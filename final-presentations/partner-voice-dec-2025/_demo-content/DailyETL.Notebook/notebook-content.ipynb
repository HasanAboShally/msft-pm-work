{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ded479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production ETL Configuration\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Initialize Spark with production settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProductionDailyETL\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Production configuration\n",
    "PROD_CONFIG = {\n",
    "    \"environment\": \"PRODUCTION\",\n",
    "    \"run_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"source_lakehouse\": \"ProductionData\",\n",
    "    \"retry_count\": 3,\n",
    "    \"alert_on_failure\": True\n",
    "}\n",
    "\n",
    "print(f\"üè≠ Production ETL Started\")\n",
    "print(f\"üìÖ Run ID: {PROD_CONFIG['run_id']}\")\n",
    "print(f\"‚öôÔ∏è Environment: {PROD_CONFIG['environment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44cc74",
   "metadata": {},
   "source": [
    "## üîê Data Quality Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityChecker:\n",
    "    \"\"\"Production data quality validation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df, name):\n",
    "        self.df = df\n",
    "        self.name = name\n",
    "        self.checks_passed = []\n",
    "        self.checks_failed = []\n",
    "    \n",
    "    def check_row_count(self, min_rows=1):\n",
    "        \"\"\"Ensure minimum row count\"\"\"\n",
    "        count = self.df.count()\n",
    "        if count >= min_rows:\n",
    "            self.checks_passed.append(f\"Row count: {count} >= {min_rows}\")\n",
    "            return True\n",
    "        self.checks_failed.append(f\"Row count: {count} < {min_rows}\")\n",
    "        return False\n",
    "    \n",
    "    def check_null_percentage(self, column, max_null_pct=5):\n",
    "        \"\"\"Check null percentage for a column\"\"\"\n",
    "        total = self.df.count()\n",
    "        nulls = self.df.filter(col(column).isNull()).count()\n",
    "        null_pct = (nulls / total) * 100 if total > 0 else 0\n",
    "        \n",
    "        if null_pct <= max_null_pct:\n",
    "            self.checks_passed.append(f\"{column} null%: {null_pct:.2f}% <= {max_null_pct}%\")\n",
    "            return True\n",
    "        self.checks_failed.append(f\"{column} null%: {null_pct:.2f}% > {max_null_pct}%\")\n",
    "        return False\n",
    "    \n",
    "    def check_unique(self, column):\n",
    "        \"\"\"Verify column uniqueness\"\"\"\n",
    "        total = self.df.count()\n",
    "        unique = self.df.select(column).distinct().count()\n",
    "        \n",
    "        if total == unique:\n",
    "            self.checks_passed.append(f\"{column} is unique\")\n",
    "            return True\n",
    "        self.checks_failed.append(f\"{column} has duplicates: {total - unique}\")\n",
    "        return False\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate quality report\"\"\"\n",
    "        print(f\"\\nüìã Data Quality Report: {self.name}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"‚úÖ Passed: {len(self.checks_passed)}\")\n",
    "        for check in self.checks_passed:\n",
    "            print(f\"   ‚Ä¢ {check}\")\n",
    "        print(f\"‚ùå Failed: {len(self.checks_failed)}\")\n",
    "        for check in self.checks_failed:\n",
    "            print(f\"   ‚Ä¢ {check}\")\n",
    "        \n",
    "        return len(self.checks_failed) == 0\n",
    "\n",
    "print(\"‚úÖ Data Quality Framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321783c",
   "metadata": {},
   "source": [
    "## üìä Load and Validate Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production data\n",
    "df_transactions = spark.read.format(\"delta\").load(\"Tables/sales_transactions\")\n",
    "\n",
    "# Run quality checks\n",
    "checker = DataQualityChecker(df_transactions, \"Sales Transactions\")\n",
    "checker.check_row_count(min_rows=10)\n",
    "checker.check_null_percentage(\"customer_id\", max_null_pct=1)\n",
    "checker.check_null_percentage(\"total_amount\", max_null_pct=0)\n",
    "checker.check_unique(\"transaction_id\")\n",
    "\n",
    "quality_passed = checker.report()\n",
    "\n",
    "if not quality_passed:\n",
    "    raise Exception(\"‚ùå Data quality checks failed! Pipeline halted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd69d4",
   "metadata": {},
   "source": [
    "## üîÑ Incremental Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get watermark for incremental load\n",
    "try:\n",
    "    watermark_df = spark.read.format(\"delta\").load(\"Tables/etl_watermarks\")\n",
    "    last_watermark = watermark_df.filter(col(\"pipeline_name\") == \"daily_etl\") \\\n",
    "        .select(max(\"watermark_value\")).collect()[0][0]\n",
    "except:\n",
    "    last_watermark = \"1900-01-01\"\n",
    "\n",
    "print(f\"üìç Last watermark: {last_watermark}\")\n",
    "\n",
    "# Filter for new/updated records\n",
    "df_incremental = df_transactions.filter(\n",
    "    col(\"processed_timestamp\") > last_watermark\n",
    ")\n",
    "\n",
    "new_records = df_incremental.count()\n",
    "print(f\"üìà New records to process: {new_records}\")\n",
    "\n",
    "if new_records == 0:\n",
    "    print(\"‚ÑπÔ∏è No new records to process. Pipeline complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d0b6a",
   "metadata": {},
   "source": [
    "## üì§ Update Production Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated views for reporting\n",
    "if new_records > 0:\n",
    "    # Daily summary\n",
    "    daily_summary = df_transactions.groupBy(\n",
    "        to_date(col(\"transaction_date\")).alias(\"date\")\n",
    "    ).agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"total_amount\").alias(\"daily_revenue\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction\")\n",
    "    ).orderBy(\"date\")\n",
    "    \n",
    "    # Write to production tables\n",
    "    daily_summary.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"Tables/daily_revenue_summary\")\n",
    "    \n",
    "    print(\"‚úÖ Production tables updated successfully\")\n",
    "\n",
    "# Update watermark\n",
    "new_watermark = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"üìç New watermark: {new_watermark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed6963",
   "metadata": {},
   "source": [
    "## üìß Pipeline Completion Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5613735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completion report\n",
    "completion_report = {\n",
    "    \"pipeline_name\": \"Production Daily ETL\",\n",
    "    \"run_id\": PROD_CONFIG['run_id'],\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"records_processed\": new_records,\n",
    "    \"quality_checks_passed\": quality_passed,\n",
    "    \"completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üè≠ PRODUCTION ETL COMPLETION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in completion_report.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüéâ Production ETL completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
