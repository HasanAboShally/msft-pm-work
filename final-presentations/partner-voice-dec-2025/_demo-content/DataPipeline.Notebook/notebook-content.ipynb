{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0414a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ETLPipeline\")\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"DailyETLPipeline\").getOrCreate()\n",
    "\n",
    "# Pipeline configuration\n",
    "CONFIG = {\n",
    "    \"source_path\": \"Files/raw/\",\n",
    "    \"staging_path\": \"Files/staging/\",\n",
    "    \"target_path\": \"Tables/\",\n",
    "    \"run_date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "}\n",
    "\n",
    "logger.info(f\"ðŸš€ ETL Pipeline started - Run Date: {CONFIG['run_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55724298",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 1: Extract - Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for data validation\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_email\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"unit_price\", DoubleType(), False),\n",
    "    StructField(\"total_amount\", DoubleType(), False),\n",
    "    StructField(\"transaction_date\", DateType(), False),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"is_member\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract raw sales data\n",
    "logger.info(\"ðŸ“‚ Loading raw sales data...\")\n",
    "df_raw_sales = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{CONFIG['source_path']}sales_data.csv\")\n",
    "\n",
    "logger.info(f\"âœ… Loaded {df_raw_sales.count()} raw sales records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f90cac",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Step 2: Transform - Data Cleaning & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "def validate_data(df, name):\n",
    "    \"\"\"Perform data quality validation\"\"\"\n",
    "    total_rows = df.count()\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    logger.info(f\"ðŸ“Š {name} - Total rows: {total_rows}\")\n",
    "    logger.info(f\"ðŸ“Š {name} - Null counts per column:\")\n",
    "    null_counts.show()\n",
    "    return total_rows\n",
    "\n",
    "# Clean sales data\n",
    "df_clean_sales = df_raw_sales \\\n",
    "    .dropDuplicates([\"transaction_id\"]) \\\n",
    "    .filter(col(\"total_amount\") > 0) \\\n",
    "    .filter(col(\"quantity\") > 0) \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"is_member\", when(col(\"is_member\") == \"Yes\", True).otherwise(False)) \\\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"etl_batch_id\", lit(CONFIG['run_date']))\n",
    "\n",
    "# Validate cleaned data\n",
    "clean_count = validate_data(df_clean_sales, \"Cleaned Sales\")\n",
    "logger.info(f\"ðŸ§¹ Data cleaning complete - {clean_count} valid records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52f0e0",
   "metadata": {},
   "source": [
    "## ðŸ”„ Step 3: Transform - Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived columns\n",
    "df_enriched = df_clean_sales \\\n",
    "    .withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin(1, 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"profit_margin\", \n",
    "        round(col(\"total_amount\") * 0.35, 2)  # Assuming 35% margin\n",
    "    ) \\\n",
    "    .withColumn(\"customer_tier\",\n",
    "        when(col(\"total_amount\") >= 1000, \"Premium\")\n",
    "        .when(col(\"total_amount\") >= 500, \"Standard\")\n",
    "        .otherwise(\"Basic\")\n",
    "    )\n",
    "\n",
    "logger.info(\"âœ¨ Data enrichment complete\")\n",
    "df_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67778c",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 4: Load - Write to Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta table with merge/upsert\n",
    "target_table = \"sales_transactions\"\n",
    "\n",
    "logger.info(f\"ðŸ’¾ Writing to Delta table: {target_table}\")\n",
    "\n",
    "# Use merge for incremental load (upsert pattern)\n",
    "df_enriched.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .save(f\"{CONFIG['target_path']}{target_table}\")\n",
    "\n",
    "logger.info(f\"âœ… Successfully loaded {df_enriched.count()} records to {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7562f",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 5: Post-Load Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate loaded data\n",
    "df_loaded = spark.read.format(\"delta\").load(f\"{CONFIG['target_path']}{target_table}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "summary = df_loaded.agg(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction\"),\n",
    "    min(\"transaction_date\").alias(\"earliest_date\"),\n",
    "    max(\"transaction_date\").alias(\"latest_date\")\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š ETL Pipeline Summary:\")\n",
    "print(\"=\" * 50)\n",
    "summary.show(truncate=False)\n",
    "\n",
    "logger.info(\"ðŸŽ‰ ETL Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
