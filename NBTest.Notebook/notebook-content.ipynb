{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Lakehouse Data Summary Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Read data from a Microsoft Fabric Lakehouse\n",
    "2. Explore and summarize the data\n",
    "3. Generate basic statistics and visualizations\n",
    "\n",
    "**Prerequisites:**\n",
    "- Attach this notebook to a Lakehouse in your Fabric workspace\n",
    "- Ensure you have data tables available in the Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ff99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum, min, max, desc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03f2c3",
   "metadata": {},
   "source": [
    "## Step 1: List Available Tables in Lakehouse\n",
    "\n",
    "First, let's discover what tables are available in the attached Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the default lakehouse\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "display(tables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8426b35",
   "metadata": {},
   "source": [
    "## Step 2: Read Data from Lakehouse\n",
    "\n",
    "Read a Delta table from the Lakehouse into a Spark DataFrame. Replace `your_table_name` with an actual table from your Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these values with your table name\n",
    "TABLE_NAME = \"your_table_name\"  # Replace with your actual table name\n",
    "\n",
    "# Option 1: Read using spark.read.table (recommended for Delta tables)\n",
    "df = spark.read.table(TABLE_NAME)\n",
    "\n",
    "# Option 2: Read from Delta path directly\n",
    "# df = spark.read.format(\"delta\").load(\"Tables/\" + TABLE_NAME)\n",
    "\n",
    "# Option 3: Read using SQL\n",
    "# df = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"‚úÖ Successfully loaded table: {TABLE_NAME}\")\n",
    "print(f\"üìä Total records: {df.count():,}\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f5be2",
   "metadata": {},
   "source": [
    "## Step 3: Explore Data Schema\n",
    "\n",
    "Understanding the structure of your data is crucial for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2dbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema information\n",
    "print(\"üìã Data Schema:\")\n",
    "print(\"-\" * 50)\n",
    "df.printSchema()\n",
    "\n",
    "# Get column names and types\n",
    "print(\"\\nüìù Column Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for field in df.schema.fields:\n",
    "    print(f\"  ‚Ä¢ {field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb266cc2",
   "metadata": {},
   "source": [
    "## Step 4: Generate Statistical Summary\n",
    "\n",
    "Get descriptive statistics for all numeric columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d497da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for all columns\n",
    "print(\"üìà Statistical Summary:\")\n",
    "print(\"-\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "# Additional summary using pandas for more detailed stats\n",
    "pandas_df = df.toPandas()\n",
    "print(\"\\nüìä Detailed Statistics (via Pandas):\")\n",
    "display(pandas_df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5dc74",
   "metadata": {},
   "source": [
    "## Step 5: Data Quality Check\n",
    "\n",
    "Check for missing values and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "print(\"üîç Null Value Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Count nulls for each column\n",
    "null_counts = df.select([\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "display(null_counts)\n",
    "\n",
    "# Calculate null percentages\n",
    "total_rows = df.count()\n",
    "print(f\"\\nüìä Total Rows: {total_rows:,}\")\n",
    "print(\"\\nüéØ Null Percentage by Column:\")\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "    print(f\"  ‚Ä¢ {column}: {null_pct:.2f}% ({null_count:,} nulls)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574281b",
   "metadata": {},
   "source": [
    "## Step 6: Generate Summary Report\n",
    "\n",
    "Create a comprehensive summary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data summary report\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "def generate_data_summary(dataframe, table_name):\n",
    "    \"\"\"Generate a comprehensive summary report for a DataFrame\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä DATA SUMMARY REPORT: {table_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic Info\n",
    "    print(f\"\\nüìå Basic Information:\")\n",
    "    print(f\"   ‚Ä¢ Total Records: {dataframe.count():,}\")\n",
    "    print(f\"   ‚Ä¢ Total Columns: {len(dataframe.columns)}\")\n",
    "    \n",
    "    # Column Types\n",
    "    numeric_cols = [f.name for f in dataframe.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "    string_cols = [f.name for f in dataframe.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    \n",
    "    print(f\"\\nüìã Column Types:\")\n",
    "    print(f\"   ‚Ä¢ Numeric Columns: {len(numeric_cols)}\")\n",
    "    print(f\"   ‚Ä¢ String Columns: {len(string_cols)}\")\n",
    "    print(f\"   ‚Ä¢ Other Columns: {len(dataframe.columns) - len(numeric_cols) - len(string_cols)}\")\n",
    "    \n",
    "    # Memory estimate\n",
    "    print(f\"\\nüíæ Estimated Memory Usage:\")\n",
    "    row_count = dataframe.count()\n",
    "    col_count = len(dataframe.columns)\n",
    "    estimated_mb = (row_count * col_count * 8) / (1024 * 1024)  # rough estimate\n",
    "    print(f\"   ‚Ä¢ Approximate: {estimated_mb:.2f} MB\")\n",
    "    \n",
    "    # Numeric summaries\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nüìà Numeric Column Statistics:\")\n",
    "        for col_name in numeric_cols[:5]:  # Limit to first 5 numeric columns\n",
    "            stats = dataframe.agg(\n",
    "                min(col_name).alias(\"min\"),\n",
    "                max(col_name).alias(\"max\"),\n",
    "                avg(col_name).alias(\"avg\"),\n",
    "                sum(col_name).alias(\"sum\")\n",
    "            ).collect()[0]\n",
    "            print(f\"   ‚Ä¢ {col_name}:\")\n",
    "            print(f\"      Min: {stats['min']}, Max: {stats['max']}, Avg: {stats['avg']:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Summary report generated successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generate the summary report\n",
    "generate_data_summary(df, TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8d1c1",
   "metadata": {},
   "source": [
    "## Step 7: Save Summary Results (Optional)\n",
    "\n",
    "Optionally save the summary statistics back to the Lakehouse as a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7af4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics to a new Delta table in the Lakehouse\n",
    "SUMMARY_TABLE_NAME = f\"{TABLE_NAME}_summary\"\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Save to Lakehouse as Delta table\n",
    "summary_stats.write.mode(\"overwrite\").format(\"delta\").saveAsTable(SUMMARY_TABLE_NAME)\n",
    "\n",
    "print(f\"‚úÖ Summary statistics saved to table: {SUMMARY_TABLE_NAME}\")\n",
    "print(f\"üìç Location: Tables/{SUMMARY_TABLE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
