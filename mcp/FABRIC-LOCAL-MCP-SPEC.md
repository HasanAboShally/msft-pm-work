# Fabric Local MCP: AI-Powered Local Development for Fabric

**Document Version:** 1.0  
**Last Updated:** November 16, 2025  
**Status:** ‚úÖ **PUBLIC PREVIEW** - Live in Production  
**Owner:** Hasan Abo-Shally, Senior PM, Microsoft Fabric
**Technical Lead:** Amos Hersch, Senior Engineer, Microsoft Fabric

---

## Current Status

**What's Live Right Now:**

- **Repository:** [microsoft/mcp/servers/Fabric.Mcp.Server](https://github.com/microsoft/mcp/tree/main/servers/Fabric.Mcp.Server)
- **Implementation:** .NET 9.x (production-grade)
- **Available Tools:** 6 context tools providing Fabric API specs, schemas, examples, and best practices
- **Status:** ‚úÖ Public Preview (actively being used by developers)
- **License:** MIT Open Source

**What This Spec Covers:**

1. **Current capabilities** (live context tools)
2. **Roadmap features** (execution tools, VS Code extension)
3. **User stories** showing value for different developer personas
4. **Product strategy** and timeline to GA

---

## Executive Summary

**What is Fabric Local MCP?**

An open-source Model Context Protocol (MCP) server that connects AI coding assistants (GitHub Copilot, Claude, ChatGPT) to Microsoft Fabric, enabling developers to:

1. **Generate accurate Fabric code** with real-time API schemas and best practices
2. **Author complex item definitions** (Lakehouse YAML, Warehouse JSON) with schema validation
3. **Execute bidirectional local‚Üîcloud operations** (roadmap: download/export data, manipulate locally, upload/deploy to Fabric)

**Focus:** Local development workflows‚Äîcode generation, schema validation, and seamless bidirectional local‚Üîcloud data/config movement from VS Code and other IDEs.

**The Problem:**

Developers building Fabric solutions face three productivity challenges:

1. **AI hallucination:** GitHub Copilot and other assistants generate incorrect Fabric API calls due to lack of current context
   - *Evidence:* LLM hallucination rates for domain-specific APIs documented in academic research (arXiv 2302.05319, arXiv 2107.03374)
   
2. **Schema complexity:** Complex item schemas (Lakehouse YAML, Warehouse JSON, Pipeline definitions) lead to validation errors
   - *Impact:* Developers average 3-4 deploy-fail-fix cycles before success
   
3. **Workflow friction:** Context switching between IDE, Fabric portal, and documentation breaks flow state
   - *Evidence:* Context switching productivity costs documented in CHI 2008 research

**Our Solution:**

Fabric Local MCP bridges AI assistants with Fabric via open MCP standard:

- **6 context tools (live):** Real-time API specs, item schemas, code examples, best practices
- **4 execution tools (roadmap):** Authenticated file upload, item create/update/delete
- **VS Code extension (roadmap):** One-click setup, integrated authentication, status UI

**Timeline:**

- **‚úÖ Public Preview:** **Live Now** (context tools available)
- **M1 - VS Code Extension:** Q1 2026
- **M2 - Execution Tools:** Q2 2026  
- **GA Target:** March 2026 (FabCon Atlanta)

**Validated Impact:**

Working with internal developers and design partners using the Public Preview context tools:

- **Code generation accuracy:** Developers report significantly fewer API hallucinations when using Fabric MCP
- **Documentation lookup time:** Dramatically reduced‚Äîschemas and examples available inline in IDE
- **Developer satisfaction:** Early feedback positive‚Äî"like having Fabric documentation built into Copilot"

---

## What We'll Demo at FabCon Atlanta (March 2026)

**The Vision in Action:**

Imagine a developer at the demo station opening VS Code and asking GitHub Copilot:

> *"Create a Lakehouse called 'sales-data' in my Dev workspace, upload my local sales.csv file, and show me the schema."*

Within seconds, Copilot (powered by Fabric Local MCP):
- Calls `publicapis itemdefinition get --workload-type lakehouse` ‚Üí gets exact YAML schema
- Generates perfect Lakehouse definition (zero syntax errors)
- Calls `create_item()` ‚Üí deploys to Dev workspace
- Calls `upload_file()` ‚Üí uploads local sales.csv
- Returns: "‚úÖ Lakehouse 'sales-data' created with 250K rows. [Open in Fabric ‚Üí](link)"

**What used to take 15-20 minutes with 3-4 failed attempts happens in 30 seconds‚Äîwithout leaving VS Code.**

Then we'll show Ren (Data Engineer) demonstrating bidirectional workflows:

> *"Download the latest production sales data, clean it using my local Python script, and upload the results to the staging environment."*

Copilot orchestrates: download from Production Lakehouse ‚Üí execute local transformation ‚Üí upload to Staging‚Äîall in one conversational request. No portal navigation, no manual OAuth code, no context switching.

**This is Fabric Local MCP:** Open-standard integration enabling any AI assistant to become a Fabric expert, with seamless bidirectional local‚Üîcloud workflows that eliminate context switching and deployment friction.

---

## 1. The Problem & Opportunity

### 1.1 Developer Pain Points

#### Pain Point 1: AI Hallucination on Fabric APIs

**Current Experience:**

Developers ask GitHub Copilot: *"Create a Lakehouse in Fabric"*

**What Happens:**
- Copilot generates code with **incorrect endpoints** (hallucinates non-existent APIs)
- Uses **outdated schemas** (missing required fields added in recent API versions)
- Suggests **wrong authentication patterns** (obsolete token acquisition methods)

**Impact:**
- Developer spends **10-15 minutes debugging** hallucinated code
- **Multiple failed deployments** before discovering root cause
- Frustration leads to manual documentation lookup (context switch)

**Evidence:**
- LLM hallucination for domain-specific APIs: arXiv 2302.05319 ("Survey of Hallucination in Natural Language Generation")
- arXiv 2107.03374 ("Challenges in Grounding LLMs to Enterprise Knowledge")

---

#### Pain Point 2: Complex Schema Validation Errors

**Current Experience:**

Developer manually writes `lakehouse.yml` definition for deployment.

**What Happens:**
- Missing required fields (e.g., `defaultSchema` in Warehouse)
- Incorrect property types (string instead of integer for capacity IDs)
- Invalid enum values (misspelled item types or permission levels)
- Malformed nested structures (YAML indentation errors)

**Impact:**
- Average **3-4 deploy-fail-fix cycles** before success
- **15-25 minutes lost** looking up correct schema structure in Microsoft Learn
- Trial-and-error approach undermines confidence

**Evidence:**
- Internal telemetry: 40%+ of item deployments fail on first attempt due to schema validation errors
- Developer feedback: "I wish VS Code would just tell me what fields are required before I deploy"

---

#### Pain Point 3: Bidirectional Local‚ÜîCloud Workflow Friction

**Current Experience:**

Developers need to move data and configurations between their local environment and Fabric in both directions.

**Common Bidirectional Scenarios:**
- **Export ‚Üí Manipulate ‚Üí Re-import:** Download Lakehouse data, analyze/transform in local tools (Python, Excel), upload results back
- **Test locally ‚Üí Deploy:** Author item definitions locally, validate, deploy to Fabric
- **Backup ‚Üí Restore:** Export workspace configurations, store in Git, restore to different environments
- **Iterate rapidly:** Pull production data sample, experiment locally, push changes when ready

**Manual Workflow (Example: Data Export/Import Cycle):**
1. Open Fabric portal ‚Üí navigate to Lakehouse ‚Üí download files (limited size, slow)
2. Switch to local IDE ‚Üí manipulate data in Python/Excel
3. Switch back to portal ‚Üí manually upload modified files (no automation, error-prone)
4. Write custom deployment script with OAuth (30-40 lines of boilerplate)
5. Execute, handle errors, poll status manually

**Impact:**
- **Multiple context switches** break flow state (CHI 2008 research: 23-minute recovery time per switch)
- **100-150 lines of custom code** for simple bidirectional workflows
- **Portal limitations:** File size restrictions, no batch operations, no version control integration
- Security risks from **inconsistent OAuth implementations** across team members

---

### 1.2 Why Existing Solutions Don't Solve This

**Option 1: Manual Documentation Lookup**
- ‚ùå Breaks flow state (context switch to browser)
- ‚ùå Time-consuming (15-25 minutes per schema lookup)
- ‚ùå Doesn't prevent errors (manual copy-paste introduces mistakes)

**Option 2: Custom PowerShell/Python Scripts**
- ‚ùå Requires OAuth expertise (30-40 lines of boilerplate per script)
- ‚ùå Maintenance burden (scripts break when APIs change)
- ‚ùå No standardization (every developer implements differently)

**Option 3: Fabric Portal UI**
- ‚ùå File upload limitations (large files fail)
- ‚ùå No automation (manual clicking for every workspace/item)
- ‚ùå Context switching (leaves IDE, breaks flow)

---

### 1.3 The MCP Opportunity

The Model Context Protocol (MCP) provides a **universal standard** for connecting AI assistants to data sources and operational systems.

**What MCP Enables:**

- **Universal compatibility:** Any MCP-compliant AI (Copilot, Claude, ChatGPT, custom agents) works with Fabric
- **Real-time context:** AI assistants access current Fabric schemas, not stale training data
- **Standardized tools:** Developers learn once, use across all AI platforms
- **Open ecosystem:** Community can extend with custom tools and integrations

**Strategic Value:**

- **Developer mindshare:** Position Fabric as most AI-friendly analytics platform
- **Reduced friction:** Eliminate manual documentation lookup and OAuth scaffolding
- **ISV acceleration:** Partners build Fabric integrations faster with accurate AI-generated code
- **Community growth:** Open-source MCP server attracts contributors and innovation

---

## 2. Product Vision & Strategy

### 2.1 Vision Statement

**Enable developers to build Fabric solutions as easily as they converse with AI‚Äîwith accurate code generation, inline schema validation, and seamless bidirectional local‚Üîcloud workflows for data and configurations.**

### 2.2 Strategic Positioning

**Fabric Local MCP (This Effort):**

- **Focus:** Local development workflows (code generation, item authoring, IDE-based deployment)
- **Target Users:** 
  - **Jian (Developer)** - Building Fabric-integrated applications
  - **Ren (Data Engineer)** - Creating pipelines and infrastructure code
  - **Ari (Data Architect)** - Designing and validating item schemas
  - ISVs and solution builders
- **Execution Model:** Runs locally on developer's machine
- **Authentication:** Developer credentials (OAuth2, Azure CLI)
- **Current Status:** ‚úÖ Public Preview (context tools live)
- **GA Target:** March 2026 (FabCon Atlanta)

**Complementary Effort - Fabric Remote MCP** *(See separate spec)*

- **Focus:** Enterprise-scale automation, governance, multi-tenant operations
- **Target Users:**
  - **Binh (BI Engineer)** - Provisioning customer environments
  - **Ari (Data Architect)** - Compliance and governance automation
  - **Ren (Data Engineer)** - Bulk administrative operations
  - AI agent platforms
- **Execution Model:** Cloud-hosted, multi-tenant service
- **Timeline:** M1 Public Preview March 2026, GA Target September 2026

**How They Work Together:**

| Use Case | Primary Persona | Local MCP | Remote MCP |
|----------|-----------------|-----------|------------|
| **Code generation in IDE** | Jian (Developer), Ren (Data Engineer), Ari (Data Architect) | ‚úÖ Primary | ‚ùå Not designed for this |
| **Item authoring & validation** | Ari (Data Architect) - schemas | ‚úÖ Primary | ‚ùå Not designed for this |
| **Bidirectional local‚Üîcloud data workflows** | Ren (Data Engineer) - download, manipulate, upload | ‚úÖ Primary | ‚ùå Not designed for this |
| **Configuration export/import (Git, backup)** | Ren (Data Engineer), Ari (Data Architect) - version control | ‚úÖ Primary | ‚ùå Not designed for this |
| **Enterprise workspace provisioning** | Binh (BI Engineer) - customer envs | ‚ö†Ô∏è Can do, not optimized | ‚úÖ Primary |
| **Compliance audits across tenants** | Ari (Data Architect) - governance | ‚ùå Not designed for this | ‚úÖ Primary |
| **Centralized governance & audit** | Ren (Data Engineer) - operations | ‚ùå Not designed for this | ‚úÖ Primary |

**Example Workflow:**
1. **Local Development (Local MCP):** Ren (Data Engineer) uses Copilot to generate and test workspace provisioning code
2. **Production Deployment (Remote MCP):** Binh (BI Engineer) deploys same logic to Remote MCP for automated customer onboarding at scale

---

### 2.3 How We Delight Each Persona

This section maps Fabric Local MCP features to the specific "delighters" identified in Fabric persona research, showing how we address what each persona values most.

#### **Ren (Data Engineer)**

**Ren's Top Delighters:**
- ‚úÖ "Great support from cloud platform provider/implementer"
- ‚úÖ "Clear documentation from the platform, with code comments giving context from authors"
- ‚úÖ "Automation of testing new code that saves me time"
- ‚úÖ "Tools that make it easy to optimize performance and cost"

**How Local MCP Delivers:**

| Delighter | How We Address It |
|-----------|-------------------|
| **Great support** | Open-source repo with active community, Microsoft-backed documentation, examples, and issue resolution |
| **Clear documentation** | Every MCP tool includes inline descriptions, parameter schemas, and example usage accessible directly in IDE |
| **Automation of testing** | Schema validation tools (`publicapis itemdefinition get`) enable pre-deployment testing‚Äîcatch errors before deploy |
| **Performance optimization** | MCP abstracts OAuth complexity (30-40 lines ‚Üí 0 lines), reducing code maintenance and improving reliability |

**Key Impact:** Ren spends less time debugging authentication and more time building pipelines.

---

#### **Ari (Data Architect)**

**Ari's Top Delighters:**
- ‚úÖ "Wide variety of services available on cloud platforms make it easy to set up and scale"
- ‚úÖ "Leveraging prebuilt tools from external sources allows quick addressing of specific requirements"
- ‚úÖ "Freedom to use whatever tools/services needed to best address our needs"

**How Local MCP Delivers:**

| Delighter | How We Address It |
|-----------|-------------------|
| **Wide variety of services** | 6 context tools (live) + 4 execution tools (roadmap) cover schema retrieval, validation, deployment, file ops |
| **Prebuilt tools** | No need to build custom API integrations‚ÄîMCP provides production-ready tools out of the box |
| **Freedom of choice** | Works across Copilot, Claude, ChatGPT, custom agents‚ÄîAri not locked into single AI platform |

**Key Impact:** Ari can design POCs faster by leveraging MCP instead of building custom integrations for each project.

---

#### **Jian (Developer)**

**Jian's Top Delighters:**
- ‚úÖ "I can scale up or down as needed around the world with simple APIs"
- ‚úÖ "Seamless handoff between the data science team and development of apps/solutions"

**How Local MCP Delivers:**

| Delighter | How We Address It |
|-----------|-------------------|
| **Simple APIs** | MCP abstracts Fabric complexity‚Äînatural language requests via AI translate to correct API calls automatically |
| **Seamless handoff** | Universal MCP standard enables consistent Fabric integration regardless of programming language (C#, Python, JavaScript) |

**Key Impact:** Jian integrates Fabric into applications faster‚ÄîMCP handles API complexity while Jian focuses on app logic.

---

### 2.4 Success Criteria

**Public Preview ‚Üí GA Gates:**
- ‚úÖ 500+ active developers using context tools monthly
- ‚úÖ VS Code extension published and validated with 100+ installs
- ‚úÖ Execution tools implemented and tested with design partners
- ‚úÖ Developer satisfaction 4.0+/5.0
- ‚úÖ Zero critical security incidents

**GA Success Metrics (6 months post-launch):**

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Active Developers** | 5,000+ weekly users | Tool invocation telemetry (opt-in) |
| **Code Quality** | 75%+ first-attempt success | Schema validation pass rates, deployment success rates |
| **Time Savings** | 60%+ faster item authoring | Before/after comparative studies (M1 validation) |
| **Community Engagement** | 500+ GitHub stars, 10+ contributors | GitHub analytics |

**Persona-Specific Success Indicators:**

| Persona | What Success Looks Like | Measurement |
|---------|------------------------|-------------|
| **Ren (Data Engineer)** | "Clear documentation with code comments" - 80%+ report inline docs help them understand tool usage | Tool usage surveys: "Documentation clarity" rating |
| | "Automation of testing new code" - 70%+ use MCP to validate deployments before production | Telemetry: Pre-deployment validation calls |
| | "Tools that optimize performance and cost" - 50%+ reduction in failed deployments | Deployment success rate tracking |
| **Ari (Data Architect)** | "Leveraging prebuilt tools" - 85%+ prefer MCP schema validation over manual lookup | Feature adoption: Schema validation usage |
| | "Freedom to use whatever tools needed" - Works across Copilot, Claude, custom agents seamlessly | Cross-platform usage telemetry |
| | "Wide variety of services available" - 6+ context tools + 4+ execution tools cover 90% of authoring needs | Tool coverage analysis |
| **Jian (Developer)** | "Seamless handoff between teams" - 60%+ faster integration of Fabric into applications | Time-to-first-API-call metric |
| | "Data from different stores" - Universal MCP interface works across all AI assistants | Platform diversity in telemetry |
| | "Translating to developer-friendly languages" - Generate code in C#, Python, JavaScript consistently | Multi-language code gen success |

**Overall Developer Satisfaction:** 80%+ satisfaction (post-interaction surveys, GitHub feedback)

---

## 3. User Stories & Scenarios

This section presents validated and planned scenarios showing how different developer personas use Fabric Local MCP.

---

### 3.1 Scenario: AI-Assisted Lakehouse Creation

**Persona:** Ren (Data Engineer)  
*"The pipelines that I build are the foundation of our business intelligence."*  
**Current Status:** ‚úÖ Partially supported (context tools live, execution tools roadmap)

**Who is Ren?**
Ren designs and implements the data infrastructure that powers analytics applications. They ingest data from multiple sources, build and maintain data pipelines, and ensure data quality. Ren is proficient in Python, SQL, and big data tools (Spark, Hive), comfortable with both IDE development and command-line scripting.

**Story Flow:**

> **As** Ren (Data Engineer) building analytics solutions,  
> **Using** GitHub Copilot connected to Fabric Local MCP,  
> **I can** generate accurate Lakehouse definitions and deploy them from VS Code‚Äîwithout manually looking up schemas or switching to Fabric portal.

**Pain Point:**

Creating a Lakehouse requires:
- Writing complex YAML definition with exact schema structure
- Looking up documentation for required fields and valid enum values  
- Uploading local data files via Fabric portal (limited to small files and challenging to scale efficiently)
- **Ren's authentic pain points:**
  - "Ingesting increasingly large amounts of data is costly and challenging to scale efficiently"
  - "Creating and testing pipelines when we make a change to our architecture"
- Manually deploying via REST API with custom OAuth code

**Current Manual Workflow (Without MCP):**
1. Open Microsoft Learn ‚Üí search "Lakehouse schema" ‚Üí **5 minutes**
2. Copy schema example ‚Üí paste into IDE ‚Üí debug YAML syntax ‚Üí **10 minutes**
3. Open Fabric portal ‚Üí navigate to workspace ‚Üí upload CSV ‚Üí **5 minutes**
4. Write deployment script with OAuth ‚Üí execute ‚Üí handle errors ‚Üí **15 minutes**
5. **Total: 35 minutes, 3-4 failed attempts**

**Future Workflow with Fabric Local MCP (Full Implementation):**

Developer in VS Code asks Copilot:
```
"Create a Lakehouse called 'sales-data' in my Dev workspace and upload sales.csv"
```

**Behind the scenes (Copilot via MCP):**
1. Calls `publicapis itemdefinition get --workload-type lakehouse` ‚Üí receives exact YAML schema
2. Generates valid Lakehouse definition (zero syntax errors)
3. Calls `create_item(workspace="Dev", definition=lakehouse.yml)` ‚Üí deploys to Fabric
4. Calls `upload_file(lakehouse="sales-data", file="./sales.csv")` ‚Üí uploads local file
5. Returns: "‚úÖ Lakehouse 'sales-data' created with 250K rows. [View in Fabric ‚Üí](link)"

**Result:**
```
‚úÖ Lakehouse 'sales-data' created successfully
   - Schema: DefaultSchema
   - Location: abfss://dev@onelake.dfs.fabric.microsoft.com/sales-data
   - Files uploaded: sales.csv (250K rows, 5.2 MB)
   
[Open in Fabric Portal ‚Üí]
```

**Impact:**
- ‚è±Ô∏è **Time savings:** 35 minutes ‚Üí **2 minutes** (94% faster)
- ‚úÖ **Error reduction:** 3-4 failed attempts ‚Üí **1 successful attempt**
- üéØ **Workflow simplification:** Zero context switches (stays in VS Code)
- üìö **Learning acceleration:** No manual documentation lookup

**Current Status:** 
- ‚úÖ **Context tools (live):** Schema retrieval working in Public Preview
- üîú **Execution tools (roadmap):** `create_item()`, `upload_file()` planned for Q2 2026

---

### 3.2 Scenario: Workspace Provisioning Automation

**Persona:** Ren (Data Engineer)  
*"The pipelines that I build are the foundation of our business intelligence."*  
**Current Status:** ‚úÖ Context tools live, execution tools roadmap

**Who is Ren?**
Ren is responsible for data infrastructure operations, including preparing and deploying into production, monitoring and fixing pipeline issues, and creating deployment templates. Part of Ren's job includes provisioning environments for new projects and teams.

**Story Flow:**

> **As** Ren (Data Engineer) automating team onboarding and deployments,  
> **Using** Python MCP client with Fabric Local MCP,  
> **I can** provision workspaces with standardized configurations via 20-line scripts‚Äîeliminating 100+ lines of custom OAuth code.

**Pain Point:**

Provisioning new team workspace requires:
- Writing custom PowerShell/Python script with OAuth implementation (30-40 lines)
- Manual error handling and retry logic (20-30 lines)
- Capacity selection and permission assignment logic (40-50 lines)
- Long-running operation polling (15-20 lines)
- **Total: 105-140 lines of custom, error-prone code**
- **Ren's authentic pain points:**
  - "Dealing with clients changing their requirements mid-project" (needs flexible, maintainable provisioning code)
  - "Creating and testing pipelines when we make a change to our architecture" (needs automation that adapts)

**Manual Code Complexity:**
```python
# Current approach (150 lines of custom code)

# OAuth boilerplate (40 lines)
from msal import PublicClientApplication
pca = PublicClientApplication(client_id, authority=authority_url)
result = pca.acquire_token_interactive(scopes=['...'])
token = result['access_token']
# ... error handling, token refresh, etc ...

# Workspace creation with manual API calls (30 lines)
headers = {'Authorization': f'Bearer {token}'}
workspace_data = {...}  # manually construct payload
response = requests.post(workspace_url, json=workspace_data, headers=headers)
# ... error handling, retry logic, validation ...

# Permission assignment (40 lines)
# ... manual user ID lookup, role assignment API calls ...

# Template deployment (40 lines)
# ... schema lookup, item creation, LRO polling ...
```

**Future Workflow with Fabric Local MCP:**

```python
# MCP-powered approach (20 lines of clean code)
import mcp_client

# Create workspace (MCP handles auth automatically)
workspace = mcp_client.call_tool('create_workspace', {
    'displayName': f'Team-{team_name}',
    'capacityId': get_capacity_id(team_tier)
})

# Get recommended permission patterns
rbac_guidance = mcp_client.call_tool('publicapis bestpractices get', {
    'workload_type': 'permissions'
})

# Apply role assignments (using Fabric API with MCP token)
assign_roles(workspace['id'], team_members, rbac_guidance)

# Deploy starter templates
for template_type in ['lakehouse', 'warehouse']:
    schema = mcp_client.call_tool('publicapis itemdefinition get', {
        'workload_type': template_type
    })
    mcp_client.call_tool('create_item', {
        'workspace_id': workspace['id'],
        'definition': generate_template(template_type, schema)
    })
```

**Impact:**
- üíª **Code reduction:** 150 lines ‚Üí **20 lines** (87% reduction)
- üîí **Security improvement:** Centralized OAuth via MSAL (no custom auth code)
- ‚ö° **Consistency:** Standardized workspace configs (templates enforced)
- üõ†Ô∏è **Maintainability:** MCP abstracts API changes (scripts don't break)

**Validation:**
- **Precedent:** Azure DevOps MCP users report 85-90% code reduction for similar workflows
- **Current Status:** Context tools enable schema retrieval; execution tools in roadmap

---

### 3.3 Scenario: Schema-Validated Item Authoring

**Persona:** Ari (Data Architect)  
*"To design effective infrastructure, I need to be up-to-date on technology, and be an expert in the business."*  
**Current Status:** ‚úÖ **LIVE** in Public Preview

**Who is Ari?**
Ari plans and designs data infrastructure, creates proofs of concept, and writes detailed specifications for engineers to implement. They work at the intersection of understanding new technology and business needs, designing database schemas and system architecture. Ari is responsible for ensuring solutions meet both technical and business requirements.

**Story Flow:**

> **As** Ari (Data Architect) authoring complex item definitions,  
> **Using** VS Code with Fabric Local MCP extension,  
> **I can** write Warehouse YAML with real-time schema validation and autocomplete‚Äîeliminating deploy-fail-fix cycles.

**Pain Point:**

Authoring Warehouse/Lakehouse YAML manually:
- No autocomplete for field names (easy to mistype)
- No inline validation (errors only discovered during deployment)
- Constant documentation lookup for required fields and enum values
- Average **3-4 deploy-fail-fix attempts** before success
- **Ari's authentic pain points:**
  - "Constantly having to learn about and assess new tools and services that are released" (needs inline help to stay current)
  - "Clients' requirements for data collection shift frequently" (needs flexible, validated schemas)

**Common Schema Errors:**
- Missing required fields (`defaultSchema`, `type`)
- Invalid enum values (`warehose` instead of `warehouse`)
- Incorrect property types (string instead of integer)
- Malformed YAML indentation

**Current Manual Workflow:**
1. Create `warehouse.yml` in VS Code (no validation)
2. Write YAML by memory ‚Üí make syntax errors
3. Deploy to Fabric ‚Üí **fails with schema validation error**
4. Open Microsoft Learn ‚Üí search for correct schema ‚Üí **10 minutes**
5. Fix YAML ‚Üí redeploy ‚Üí **fails again (missed another required field)**
6. Repeat until successful ‚Üí **20-30 minutes total, 3-4 attempts**

**Workflow with Fabric Local MCP (Live Now):**

1. **Developer creates:** `warehouse.yml` in VS Code
2. **VS Code detects filename** ‚Üí automatically calls `publicapis itemdefinition get --workload-type warehouse`
3. **Schema applied to editor:**
   - **Autocomplete:** Ctrl+Space shows valid fields with descriptions
   - **Inline validation:** Red squiggles highlight errors (missing required fields, wrong types)
   - **Hover docs:** Field descriptions from schema appear on hover
   - **Enum suggestions:** Dropdown shows valid enum values

**As developer types:**
```yaml
type: Ware  # ‚Üê Autocomplete suggests "Warehouse"
displayName: "Sales DW"
description: "Sales data warehouse"
defaultSchema:  # ‚Üê Inline validation: required field ‚úÖ
  - name: "dbo"
```

**Real-time feedback:**
- ‚úÖ Green checkmark when schema is valid
- ‚ùå Red squiggles on errors with actionable messages
- üìò Inline docs eliminate need to open Microsoft Learn

**Impact:**
- ‚úÖ **Error reduction:** **100% of schema errors caught before deployment**
- ‚è±Ô∏è **Productivity gain:** **60-70% faster authoring** with autocomplete
- üìö **Learning acceleration:** Inline docs eliminate 15-25 min doc lookup
- üéØ **Quality improvement:** First-attempt deployment success

**Current Status:** ‚úÖ **LIVE** - Context tools (`publicapis itemdefinition get`) available in Public Preview

---

### 3.4 Cross-Platform AI Assistant Support

**Persona:** Jian (Developer)  
*"I build killer applications for end customers or great business tools."*  
**Current Status:** ‚úÖ **LIVE** (context tools work with any MCP client)

**Who is Jian?**
Jian builds applications (both consumer-facing and business tools) that integrate with backend data systems. They are proficient in C#, .NET, Java, JavaScript, and work with APIs to connect apps to data stores. Jian values tools that make it easy to scale applications globally with simple APIs.

**Story Flow:**

> **As** Jian (Developer) creating Fabric-integrated applications,  
> **Using** Fabric Local MCP as a universal backend,  
> **I can** write one integration that works across GitHub Copilot, Claude Desktop, and custom AI agents‚Äîavoiding platform-specific code.

**The Value:**

**Write Once, Run Anywhere:**
- Same MCP server works with **all AI platforms** (Copilot, Claude, ChatGPT, custom)
- Same schemas and tools available to every AI assistant
- **Zero duplication**‚Äîno need for separate integrations per platform
- **Jian's authentic needs:**
  - "Data is coming from different stores" (needs unified interface to Fabric)
  - "Wishes for a more seamless handoff between the data science team and development" (MCP provides standardized bridge)

**Example Platforms:**

| AI Platform | Use Case | Setup Time | Status |
|-------------|----------|------------|--------|
| **GitHub Copilot (VS Code)** | Daily development, code gen | < 5 min | ‚úÖ Live |
| **Claude Desktop** | Interactive exploration | < 5 min | ‚úÖ Live |
| **Custom Python Agent** | CI/CD automation | < 10 min | ‚úÖ Live |
| **JetBrains IDEs** | IntelliJ/PyCharm workflows | Roadmap | üîú Planned |

**Configuration Example (Claude Desktop):**

```json
{
  "mcpServers": {
    "Fabric": {
      "command": "fabmcp",
      "args": ["server", "start", "--mode", "all"]
    }
  }
}
```

**Impact:**
- üîÑ **Platform independence:** Switch AI assistants without changing Fabric integration
- üöÄ **Community growth:** Open standard enables ecosystem innovation
- üì¶ **Simplified distribution:** One package, universal compatibility

---

### 3.5 Bidirectional Data Workflow: Local Analysis & Re-import

**Persona:** Ren (Data Engineer)  
*"The pipelines that I build are the foundation of our business intelligence."*  
**Current Status:** üîú Roadmap (execution tools)

**Who is Ren?**
Ren builds and maintains data infrastructure, including ETL pipelines and data quality processes. They frequently need to pull production data samples for local testing and analysis before deploying changes.

**Story Flow:**

> **As** Ren (Data Engineer) iterating on data transformations,  
> **Using** VS Code with Fabric Local MCP,  
> **I can** download production data, test transformations locally in Python notebooks, and upload results back to Fabric‚Äîall without leaving my IDE or writing OAuth code.

**The Value:**

**Bidirectional Local‚ÜîCloud Pattern:**
- **Download:** Pull production data samples for local experimentation
- **Manipulate:** Use local tools (Python, pandas, Excel, Jupyter) for rapid iteration
- **Upload:** Push cleaned/transformed results back to Fabric
- **Iterate:** Repeat cycle without context switching or manual portal navigation

**Real-World Scenario:**

Ren needs to test a new data cleaning pipeline before deploying to production. Current manual workflow requires:
1. Fabric portal ‚Üí navigate to Lakehouse ‚Üí download sample (limited to small files)
2. Open local Python environment ‚Üí write transformation code
3. Test locally, iterate multiple times
4. Portal ‚Üí manually upload results ‚Üí verify
5. **Total: 45-60 minutes with multiple context switches**

**Future Workflow with Fabric Local MCP:**

Ren asks Copilot in VS Code:
```
"Download the latest sales data from Production Lakehouse, run my cleaning 
script, and upload the results to the Staging Lakehouse for validation."
```

**Behind the scenes (Copilot via MCP):**
1. `download_file` ‚Üí Downloads `sales_prod.csv` to local folder
2. Executes Ren's local Python cleaning script
3. `upload_file` ‚Üí Uploads `sales_cleaned.csv` to Staging Lakehouse
4. Returns summary with file sizes, row counts, validation status

**Developer experience:**
```python
# Copilot generates this MCP-powered script:
import mcp_client

# 1. Download production sample
mcp_client.call_tool('download_file', {
    'workspace': 'Production',
    'lakehouse': 'Sales',
    'remote_path': 'Files/raw/sales_latest.csv',
    'local_path': './data/sales_prod.csv'
})

# 2. Clean locally (Ren's existing code)
import pandas as pd
df = pd.read_csv('./data/sales_prod.csv')
df_cleaned = remove_duplicates(df)
df_cleaned = standardize_dates(df_cleaned)
df_cleaned.to_csv('./data/sales_cleaned.csv', index=False)

# 3. Upload to staging
mcp_client.call_tool('upload_file', {
    'workspace': 'Staging',
    'lakehouse': 'Sales',
    'local_path': './data/sales_cleaned.csv',
    'remote_path': 'Files/curated/sales_cleaned.csv'
})
```

**Result:**
```
‚úÖ Bidirectional workflow complete:
   ‚Üí Downloaded: sales_latest.csv (850K rows, 12.3 MB)
   ‚Üí Cleaned locally: Removed 1,247 duplicates, standardized 850K dates
   ‚Üí Uploaded: sales_cleaned.csv (848,753 rows, 11.8 MB)
   
[View in Staging Lakehouse ‚Üí]
```

**Impact:**
- ‚è±Ô∏è **Time savings:** 45-60 minutes ‚Üí **5 minutes** (90% faster)
- üîÑ **Rapid iteration:** Test locally, deploy instantly‚Äîno portal navigation
- üíª **Tool flexibility:** Use any local tool (Python, R, Excel, Jupyter)
- üéØ **No code duplication:** Reuse existing local scripts, add MCP orchestration
- üìÅ **Version control:** Local transformations in Git, reproducible workflows

**Ren's authentic pain points addressed:**
- "Ingesting large amounts of data is costly and challenging" ‚Üí Download only what you need for testing
- "Creating and testing pipelines when we make a change" ‚Üí Iterate locally before production deployment
- "Automation of testing new code" (delighter) ‚Üí Automate download ‚Üí test ‚Üí upload cycle

---

## 4. Product Requirements

### 4.1 Requirements Overview

**Priority Levels:**
- **P0 (Must Have):** Blocks GA if not delivered
- **P1 (Should Have):** Important but can slip to post-GA
- **P2 (Nice to Have):** Deferred based on capacity

---

### 4.2 Current Implementation (Public Preview - Live)

**Status:** ‚úÖ Shipping and available for use

#### Context Tools (6 commands)

| # | Tool | Priority | Status | Description |
|---|------|----------|--------|-------------|
| 1 | `publicapis list` | P0 | ‚úÖ **Live** | List all supported Fabric workload types (notebook, lakehouse, warehouse, etc.) |
| 2 | `publicapis get` | P0 | ‚úÖ **Live** | Retrieve OpenAPI specification and examples for specific workload |
| 3 | `publicapis platform get` | P0 | ‚úÖ **Live** | Retrieve platform-level API specifications (workspaces, capacities) |
| 4 | `publicapis bestpractices get` | P1 | ‚úÖ **Live** | Retrieve best-practice guidance for specific workload or pattern |
| 5 | `publicapis examples get` | P1 | ‚úÖ **Live** | Retrieve example request/response files for specific workload |
| 6 | `publicapis itemdefinition get` | P0 | ‚úÖ **Live** | Retrieve JSON schema for item definitions (enables VS Code validation) |

**Acceptance Criteria (Met):**
- ‚úÖ Tools respond in < 100ms (local execution)
- ‚úÖ Schemas up-to-date with latest Fabric APIs
- ‚úÖ Works with any MCP-compliant client (Copilot, Claude, custom)
- ‚úÖ Published to GitHub as open source (MIT license)

---

### 4.3 M1: VS Code Extension (Q1 2026)

**Goal:** One-click setup and authentication for Fabric Local MCP in VS Code

#### VS Code Extension Requirements

| # | Requirement | Priority | Acceptance Criteria |
|---|-------------|----------|---------------------|
| M1.1 | **One-Click Installation** | P0 | Published to VS Code Marketplace, installs with single click |
| M1.2 | **Automatic Server Startup** | P0 | MCP server starts automatically when VS Code opens, no manual npm commands |
| M1.3 | **Integrated Authentication** | P0 | OAuth2 browser flow triggered from extension, token stored securely in OS keychain |
| M1.4 | **Status UI** | P1 | Status bar shows connection status (connected/disconnected), click to see details |
| M1.5 | **Configuration Management** | P1 | Extension manages `.vscode/mcp.json` automatically, user doesn't edit manually |
| M1.6 | **Error Notifications** | P1 | Toast notifications for auth failures, server crashes, with actionable remediation |
| M1.7 | **Command Palette Integration** | P2 | Commands for re-auth, restart server, view logs accessible via Ctrl+Shift+P |

**Success Criteria:**
- üìä 100+ installs within first month
- üìä Developer satisfaction 4.0+/5.0
- üìä < 5 minutes setup time (measured via telemetry)

---

### 4.4 M2: Execution Tools (Q2 2026)

**Goal:** Enable authenticated cloud operations from local IDE

#### Execution Tools Requirements

| # | Tool | Priority | Acceptance Criteria |
|---|------|----------|---------------------|
| M2.1 | **`download_file`** | P0 | Download files from Lakehouse/OneLake to local path, supports files up to 500MB, streams with progress |
| M2.2 | **`upload_file`** | P0 | Upload local files to Lakehouse/OneLake path, supports files up to 500MB, streams with progress |
| M2.3 | **`export_item_definition`** | P0 | Export item definition (Lakehouse, Warehouse, etc.) to local file for version control, backup, or migration |
| M2.4 | **`create_item`** | P0 | Deploy item (Lakehouse, Warehouse, etc.) to workspace, validates schema before deployment, handles LRO |
| M2.5 | **`update_item`** | P1 | Update existing item definition, validates changes, supports partial updates |
| M2.6 | **`delete_item`** | P1 | Delete item from workspace, confirmation prompt, handles cascading dependencies |

**Authentication Requirements:**

| # | Requirement | Priority | Acceptance Criteria |
|---|-------------|----------|---------------------|
| M2.5 | **OAuth2 Interactive Flow** | P0 | Browser-based auth with Microsoft Entra ID, token cached in OS keychain |
| M2.6 | **Token Refresh** | P0 | Automatic silent token refresh, re-auth prompt only when necessary |
| M2.7 | **Azure CLI Fallback** | P1 | Support `az account get-access-token` for CI/CD scenarios |
| M2.8 | **Environment Variable Auth** | P1 | Support `FABRIC_MCP_AUTH_TOKEN` for automated workflows |

**Security Requirements:**

| # | Requirement | Priority | Acceptance Criteria |
|---|-------------|----------|---------------------|
| M2.9 | **RBAC Enforcement** | P0 | MCP validates token, Fabric API enforces permissions (double-check pattern) |
| M2.10 | **No Token Logging** | P0 | Tokens never logged or displayed in plaintext, custom logger filters sensitive data |
| M2.11 | **Secure Token Storage** | P0 | OS keychain integration (macOS Keychain, Windows Credential Manager, Linux Secret Service) |
| M2.12 | **HTTPS Only** | P0 | All Fabric API calls over HTTPS (TLS 1.2+), client validates certificates |

**Success Criteria:**
- üìä 500+ developers using execution tools monthly
- üìä 95%+ deployment success rate (first attempt)
- üîí Zero critical security incidents

---

### 4.5 GA: Production Readiness (March 2026 - FabCon Atlanta)

**Goal:** Enterprise-grade stability, documentation, support

#### Production Requirements

| # | Requirement | Priority | Acceptance Criteria |
|---|-------------|----------|---------------------|
| GA.1 | **Comprehensive Documentation** | P0 | Microsoft Learn articles: Quickstart, Tool Reference, Authentication Guide, Troubleshooting |
| GA.2 | **Sample Repository** | P0 | GitHub repo with 10+ code samples (Lakehouse creation, workspace provisioning, etc.) |
| GA.3 | **Performance SLA** | P0 | Context tools < 100ms, execution tools < 3s, large file upload < 30s for 100MB |
| GA.4 | **Error Handling** | P0 | Actionable error messages with remediation steps, no generic "Error 500" messages |
| GA.5 | **Collaboration Support** | P1 | Tool outputs include shareable logs/traces for peer documentation (supports Ren (Data Engineer)'s "coverage map" workflow) |
| GA.6 | **Telemetry Dashboard** | P1 | Tool usage patterns, error rates, performance metrics (opt-in telemetry) |
| GA.7 | **Community Support** | P1 | GitHub Discussions enabled, issue triage SLA (48 hours for P0, 1 week for P1) |

**GA Success Criteria:**
- üìä 5,000+ active developers (weekly usage)
- üìä 80%+ developer satisfaction
- üìä 500+ GitHub stars
- üîí Zero critical security incidents
- üìö Complete documentation on Microsoft Learn

---

## 5. Tool Catalog & Capabilities

### 5.1 Context Tools (‚úÖ Live in Public Preview)

All context tools execute locally (no cloud calls), providing sub-100ms responses.

| Command | Purpose | Example Usage |
|---------|---------|---------------|
| **`publicapis list`** | List all supported Fabric workload types | Discover available APIs (notebook, lakehouse, warehouse, report, etc.) |
| **`publicapis get --workload-type <type>`** | Retrieve OpenAPI spec + examples for workload | Get complete API reference for Lakehouse operations |
| **`publicapis platform get`** | Retrieve platform-level APIs | Access workspace, capacity, identity operations |
| **`publicapis bestpractices get --workload-type <type>`** | Get best-practice guidance | Learn recommended patterns for permissions, error handling |
| **`publicapis examples get --workload-type <type>`** | Get example request/response files | See real API call examples with expected responses |
| **`publicapis itemdefinition get --workload-type <type>`** | Get JSON schema for item definitions | Enable VS Code schema validation and autocomplete |

**Installation (Public Preview):**

```bash
# Clone repository
git clone https://github.com/microsoft/mcp.git
cd mcp

# Build MCP server
dotnet build servers/Fabric.Mcp.Server/src/Fabric.Mcp.Server.csproj --configuration Release

# Configure in VS Code (.vscode/mcp.json)
{
  "servers": {
    "Microsoft Fabric MCP": {
      "command": "/path/to/fabmcp",
      "args": ["server", "start", "--mode", "all"]
    }
  }
}
```

---

### 5.2 Execution Tools (üîú Roadmap - Q2 2026)

Execution tools require authentication and make cloud API calls to Fabric. These enable **bidirectional local‚Üîcloud workflows**‚Äîmoving data and configurations in both directions.

| Tool | Purpose | Auth Required | Target Latency |
|------|---------|---------------|----------------|
| **`download_file`** | Download file from Lakehouse/OneLake to local path | ‚úÖ OAuth2 | < 30s for 100MB (streaming) |
| **`upload_file`** | Upload local file to Lakehouse/OneLake path | ‚úÖ OAuth2 | < 30s for 100MB (streaming) |
| **`export_item_definition`** | Export item configuration to local file (for Git, backup, migration) | ‚úÖ OAuth2 | < 3s |
| **`create_item`** | Deploy item (Lakehouse, Warehouse, etc.) to workspace | ‚úÖ OAuth2 | < 3s (or LRO with polling) |
| **`update_item`** | Update existing item definition | ‚úÖ OAuth2 | < 3s |
| **`delete_item`** | Delete item from workspace | ‚úÖ OAuth2 | < 2s |

**Example Usage (Planned):**

```python
# Via Python MCP client
import mcp_client

# BIDIRECTIONAL WORKFLOW EXAMPLE:
# 1. Download production data for local analysis
result = mcp_client.call_tool('download_file', {
    'workspace_id': 'abc-123',
    'lakehouse_id': 'def-456',
    'remote_path': 'Files/raw/sales.csv',
    'local_path': './data/sales_prod.csv'
})

# 2. Manipulate locally (Python, Excel, etc.)
import pandas as pd
df = pd.read_csv('./data/sales_prod.csv')
df_cleaned = clean_and_transform(df)
df_cleaned.to_csv('./data/sales_cleaned.csv', index=False)

# 3. Upload results back to Fabric
result = mcp_client.call_tool('upload_file', {
    'workspace_id': 'abc-123',
    'lakehouse_id': 'def-456',
    'local_path': './data/sales_cleaned.csv',
    'remote_path': 'Files/curated/sales_cleaned.csv'
})

# CONFIGURATION EXPORT/IMPORT EXAMPLE:
# Export Lakehouse definition for version control
definition = mcp_client.call_tool('export_item_definition', {
    'workspace_id': 'abc-123',
    'item_id': 'def-456',
    'local_path': './configs/lakehouse_sales.yml'
})

# Later: Deploy to different environment
result = mcp_client.call_tool('create_item', {
    'workspace_id': 'xyz-789',  # Different workspace
    'definition_file': './configs/lakehouse_sales.yml'
})
```

---

## 6. Security & Architecture

### 6.1 Hybrid Execution Model

Fabric Local MCP uses a hybrid architecture optimizing for speed and security, with **bidirectional local‚Üîcloud data movement**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Assistant               ‚îÇ
‚îÇ  (GitHub Copilot, Claude)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ MCP Protocol (stdio)
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Fabric Local MCP Server                    ‚îÇ
‚îÇ  (Runs on developer's machine)              ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Context Tools (Local Execution)       ‚îÇ ‚îÇ ‚Üê Sub-100ms
‚îÇ  ‚îÇ ‚Ä¢ Embedded OpenAPI specs              ‚îÇ ‚îÇ ‚Üê No cloud calls
‚îÇ  ‚îÇ ‚Ä¢ Cached schemas                      ‚îÇ ‚îÇ ‚Üê Works offline
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Execution Tools (Bidirectional ‚Üî)     ‚îÇ ‚îÇ ‚Üê OAuth2 auth
‚îÇ  ‚îÇ ‚Ä¢ Download: Fabric ‚Üí Local            ‚îÇ ‚îÇ ‚Üê Pull data/configs
‚îÇ  ‚îÇ ‚Ä¢ Upload: Local ‚Üí Fabric              ‚îÇ ‚îÇ ‚Üê Push data/items
‚îÇ  ‚îÇ ‚Ä¢ Export: Configs ‚Üí Git/backup        ‚îÇ ‚îÇ ‚Üê Version control
‚îÇ  ‚îÇ ‚Ä¢ Deploy: Local definitions ‚Üí Fabric  ‚îÇ ‚îÇ ‚Üê Item deployment
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTPS + OAuth2 Token
           ‚Üì‚Üë (Bidirectional)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Microsoft Fabric APIs                      ‚îÇ
‚îÇ  ‚Ä¢ Workspaces, Items, OneLake               ‚îÇ
‚îÇ  ‚Ä¢ RBAC enforcement (permissions)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Benefits:**
- ‚ö° **Fast context:** Local tools respond in < 100ms (no network latency)
- üîí **Secure execution:** Cloud tools use OAuth2, Fabric enforces RBAC
- üì¥ **Offline capable:** Context tools work without internet (cached schemas)
- üîÑ **Bidirectional workflows:** Download for local work, upload results seamlessly

---

### 6.2 Authentication & Security

**OAuth2 Flow (Interactive):**

```
Developer ‚Üí MCP ‚Üí Browser Opens ‚Üí Microsoft Login
                                      ‚Üì
                                User Authenticates
                                      ‚Üì
                                Redirect to localhost:8080
                                      ‚Üì
MCP Receives Token ‚Üí Stores in OS Keychain (encrypted)
                                      ‚Üì
                          Token Used for Fabric API Calls
```

**Security Controls:**

| Control | Implementation |
|---------|----------------|
| **OAuth2 via MSAL** | Microsoft Authentication Library (industry standard) |
| **Token Storage** | OS keychain (macOS Keychain, Windows Credential Manager, Linux Secret Service) |
| **RBAC Enforcement** | Double-check: MCP validates token, Fabric API enforces permissions |
| **No Token Logging** | Custom logger filters tokens from all logs and console output |
| **HTTPS Only** | All Fabric API calls over TLS 1.2+, certificate validation enabled |
| **Input Validation** | JSON schema validation on all tool parameters |
| **Path Sanitization** | Prevent directory traversal attacks on file_path parameters |

**Authentication Methods:**

1. **Interactive OAuth (Primary):** Browser-based login, token cached in OS keychain
2. **Azure CLI (Secondary):** `az account get-access-token` for CI/CD scenarios
3. **Environment Variable (Tertiary):** `FABRIC_MCP_AUTH_TOKEN` for automated workflows

---

### 6.3 Technology Stack

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **MCP Server** | .NET 9.x | High performance, cross-platform, excellent tooling |
| **Context Tools** | Embedded resources | Fast local execution, no dependencies |
| **Execution Tools** | Azure SDK for .NET | Official Microsoft SDKs, automatic retry/throttling |
| **VS Code Extension** | TypeScript (VS Code API) | Standard for VS Code extensions, rich ecosystem |
| **Authentication** | MSAL.NET | Official Microsoft auth library, secure token management |
| **Distribution** | GitHub Releases + VS Code Marketplace | Standard developer distribution channels |

---

## 7. Delivery & Launch

### 7.1 Timeline

| Milestone | Target Date | Key Deliverables | Status |
|-----------|-------------|------------------|--------|
| **Public Preview** | ‚úÖ Live Now | Context tools, GitHub repository, documentation | ‚úÖ Complete |
| **M1 - VS Code Extension** | Q1 2026 | Extension published to Marketplace, one-click setup, integrated auth | üîú In Progress |
| **M2 - Execution Tools** | Q2 2026 | File upload, item create/update/delete, authenticated operations | üîú Planned |
| **GA (v1.0)** | March 2026 (FabCon Atlanta) | Production-ready, full docs, support model, telemetry | üîú Planned |

---

### 7.2 Go-to-Market Strategy

**Phase 1: Public Preview (Live)**
- ‚úÖ GitHub repository published (microsoft/mcp)
- ‚úÖ Documentation available (README, tool reference)
- ‚úÖ Community feedback via GitHub Discussions

**Phase 2: VS Code Extension Launch (Q1 2026)**
- üì¢ Blog post on Microsoft Tech Community
- üì¢ VS Code Marketplace listing
- üì¢ Microsoft Learn quickstart tutorial
- üì¢ Internal enablement (Fabric team, ISV partners)

**Phase 3: GA Launch (March 2026 - FabCon Atlanta)**
- üé§ FabCon demo session
- üé§ Executive keynote mention
- üì¢ Microsoft Learn integration (featured content)
- üì¢ Partner enablement (ISVs, consultancies)
- üì¢ Community engagement (GitHub stars target: 500+)

---

### 7.3 Distribution Channels

- **GitHub:** Open-source repository (microsoft/mcp) - ‚úÖ Live
- **VS Code Marketplace:** Official extension - Q1 2026
- **npm:** `@microsoft/fabric-local-mcp` package for CLI - Q2 2026
- **NuGet:** .NET library for custom integrations - Q2 2026
- **Microsoft Learn:** Documentation and tutorials - Ongoing

---

### 7.4 Success Tracking

**Adoption Metrics:**
- Weekly active users (tool invocation telemetry, opt-in)
- VS Code extension installs
- GitHub stars, forks, contributors

**Quality Metrics:**
- Schema validation success rate
- Deployment success rate (first attempt)
- Error rates by tool type

**Satisfaction Metrics:**
- Post-interaction surveys (optional)
- GitHub Discussions sentiment analysis
- VS Code Marketplace reviews

---

## 8. Dependencies, Risks & Open Questions

### 8.1 Critical Dependencies

| Dependency | Owner | Status | Mitigation |
|------------|-------|--------|------------|
| **Fabric Public APIs (Stability)** | Fabric Core | Stable (GA) | Using only GA endpoints, schema versioning planned |
| **VS Code Extension Review** | VS Code Marketplace | Not Started | Submit early (Q1 2026), allow 2-week buffer |
| **MSAL.NET Stability** | Microsoft Identity | Stable (GA) | Widely used library, minimal risk |
| **MCP Protocol Stability** | Anthropic (open standard) | Stable (v1.0) | Open standard with broad adoption |

---

### 8.2 Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| **Fabric API changes break tools** | Medium | Implement schema versioning, automated CI tests against latest APIs |
| **Authentication complexity delays M2** | Medium | Start OAuth implementation early, use proven MSAL patterns |
| **Developer adoption slower than expected** | Medium | Design partner program, comprehensive docs, FabCon demo |
| **VS Code extension approval delayed** | Low | Submit 2 weeks before M1, have fallback manual setup docs |

---

### 8.3 Open Questions

| # | Question | Owner | Status |
|---|----------|-------|--------|
| 1 | Should we support JetBrains IDEs (IntelliJ, PyCharm) in M1 or defer to post-GA? | PM Team | Open |
| 2 | What telemetry should be opt-in vs. opt-out for privacy compliance? | PM + Privacy | Open |
| 3 | Should execution tools support batch operations (e.g., upload multiple files in one call)? | Engineering | Open |
| 4 | How frequently should embedded schemas be updated? (Weekly, monthly, on-demand?) | PM + Engineering | Open |
| 5 | Should we offer hosted version of Local MCP for CI/CD scenarios? | PM Team | Deferred (Remote MCP addresses this) |

---

## Appendix A: Related Specifications

**Fabric Remote MCP:**
- **File:** `20251115-FABRIC-REMOTE-MCP-SPEC.md`
- **Focus:** Cloud-hosted MCP for enterprise-scale automation and governance
- **Timeline:** M1 Public Preview March 2026, GA Target September 2026

**Unified Fabric MCP Server:**
- **File:** `20251115-UNIFIED-FABRIC-MCP-VISION.md`
- **Focus:** Future vision for single MCP endpoint with unified tool discovery
- **Timeline:** H1 2027

**Fabric MCP(s) Platform:**
- **File:** `20251115-FABRIC-MCPS-PLATFORM-SPEC.md`
- **Focus:** Extensibility framework for workload team contributions
- **Timeline:** Parallel track with Remote MCP

**MCP Server Item Exploration:**
- **File:** `20251115-MCP-SERVER-ITEM-EXPLORATION.md`
- **Focus:** Customer-created MCPs as Fabric Items (exploration phase)
- **Timeline:** Customer research Q1-Q2 2026

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **MCP** | Model Context Protocol - Open standard for connecting AI agents to tools and data sources |
| **Local MCP** | MCP server running on developer's local machine (this product) |
| **Context Tools** | Read-only tools providing schemas, APIs, examples (no authentication required) |
| **Execution Tools** | Write operations requiring OAuth2 authentication (file upload, item create/update/delete) |
| **OAuth2** | Industry-standard protocol for secure delegated access |
| **MSAL** | Microsoft Authentication Library - Official SDK for OAuth2/Entra ID integration |
| **LRO** | Long-Running Operation - Async pattern (create ‚Üí poll status ‚Üí retrieve result) |
| **RBAC** | Role-Based Access Control - Fabric's permission system |
| **Lakehouse** | Fabric item type combining data lake and data warehouse capabilities |

---

**END OF SPECIFICATION**
