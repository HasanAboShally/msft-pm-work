````markdown
# **Fabric Local MCP – Import/Export Tools Specification (Draft)**

## **Overview & Objectives**  
**Goal:** Provide AI agents (with human oversight) the ability to **export and import Fabric item definitions** in bulk, using the Fabric Local MCP. These tools will support scenarios like workspace cloning, backup/restore, and migrating content between environments, all through **structured JSON definitions** of Fabric items. The design emphasizes **preserving relationships between items**, ensuring **user control and transparency**, and leveraging **upcoming Fabric APIs** for reliability.

**Current State:** The Fabric Local MCP (in public preview) currently exposes read-only context (e.g. listing items, retrieving schemas and definitions). It does not yet perform write actions, but upcoming enhancements (planned GA around FabCon US 2026) will enable local agents to **generate code and help author Fabric items**. Specifically, *“item definition”* support is expanding: documentation of item JSON schemas is being added as resources, and partial edit capabilities are on the roadmap. To move from suggestion to action, we’re introducing **Import** and **Export** tools that wrap Fabric’s new batch APIs for content migration.

**Planned Import/Export API:** Microsoft is releasing a **Batch Import/Export REST API** (private preview in early 2026) allowing one-call export or import of all items in a workspace. This API returns/accepts a structured JSON payload representing all item definitions. Crucially, **it auto-preserves inter-item links** (no manual ID mapping needed). Our MCP tools will integrate this API (or equivalent CLI commands) so agents can perform **one-step workspace export or import** with minimal custom logic. Internal plans confirm dedicating Q2 engineering to “add CLI command of batch import/export” and supporting query parameters for it.

## **Use Cases & Requirements**  
**Primary Use Case:** *“Workspace Clone”* – e.g., an agent clones a template workspace for a new project or client. The agent should export all definitions from source, then import them into a new workspace, **maintaining all connections** (reports pointing to the new dataset, pipelines linked to the new lakehouse, etc.). Optionally, the agent might apply minor adjustments (names, parameters) as part of the process (e.g., renaming the workspace or retargeting a data connection).  

Other use cases include:  
- **Backup & Restore:** Export a workspace’s state (as JSON definitions) for version control or backup; later import to restore or create a duplicate.  
- **Multi-Environment Deployment:** Promote content from Dev to Prod by exporting definitions from a dev workspace and importing into a prod workspace (akin to CI/CD release). This requires careful handling of references, which the new API covers via automatic re-binding of item IDs.  
- **Selective Migration:** (Future enhancement) Support exporting specific items (or a subset of a workspace) and importing them into another workspace (merging with existing content). For now, scope is primarily full workspace export/import, with potential filtering as a stretch goal.

**Functional Requirements:**  
- *R1: Export Accuracy* – The Export tool must retrieve **complete definitions of all items**, including metadata, relationships, and any configuration needed to recreate each item exactly. The export format should align with Fabric’s official schema (ensuring compatibility with the import API and CI/CD tools).  
- *R2: Import Integrity* – The Import tool must create items in the correct order and update references such that the resulting workspace is consistent. (For example, if a pipeline references a Lakehouse, the Lakehouse should be created first and the pipeline’s definition updated to the new Lakehouse’s ID automatically.) The Fabric API handles much of this *auto-binding*; the tool just needs to call it and report results.  
- *R3: All Item Types* – Support every Fabric item that has a definition. This includes Power BI artifacts (reports, datasets) and Fabric artifacts (data pipelines, Lakehouses, warehouses, notebooks, etc.). The underlying API is expected to include all these (recent updates added Lakehouse support to definition creation). The tool should not need separate logic per item type – it treats the workspace as an atomic unit for export/import.  
- *R4: Large Payload Handling* – Be able to handle workspaces with many items or big definitions. The tools should manage large JSON payloads (tens of MBs, if needed) and possibly utilize asynchronous processing. If the REST API uses long-running operations (LRO) for import, the tool should poll and only return when complete, or stream progress if supported.  
- *R5: Validation & Dry-Run* – Provide a way to validate an import without making changes. This could be a “dry-run” mode that parses the definitions, checks for obvious issues (like missing dependencies that can’t be resolved), and perhaps calls a validation endpoint if available. At minimum, the tool should catch errors from the API (e.g., if user lacks permission, or if an item already exists in target) and report them clearly.  
- *R6: User Confirmation & Control* – Since these tools can create or alter many items, they should be used by agents in a *confirm-before-execute* pattern. The agent might first use Export (read-only) freely, but before calling Import, typically it would summarize the plan (e.g., “going to create 10 items”) and get user approval. The tools themselves will not autonomously prompt, but the agent’s orchestration should include a confirmation step as per design guidelines. We will document this usage pattern for any clients of the MCP.  
- *R7: Idempotency & Conflict Handling* – Importing into an existing workspace should ideally **not duplicate items if they already exist**. The API likely requires an empty target workspace for a straightforward clone. In the future, we might allow importing updates to an existing workspace (matching by item name or ID), but initial scope assumes the target is new or user explicitly wants duplicates. Our spec will note that the first version does not handle merge scenarios; if an item with the same name exists, the import API will throw an error (and the tool will surface it).  
- *R8: Parameterization (Future)* – Consider how to handle environment-specific differences. E.g., a dataset’s connection might need to point to a prod data source instead of dev. The current API might not support on-the-fly changes, so the agent would need to adjust the JSON before import. We should allow the Import tool to accept a modified definition payload (i.e., the agent can programmatically tweak JSON between export and import). Long-term, we might integrate with Fabric’s **variable libraries** for CI/CD (mentioned by CAT team members), but for now this is an out-of-band process (the agent or user handles it manually).

## **Design & Implementation Details**  

### **Tool 1: `ExportDefinitions`**  
**Purpose:** Export the definitions of items from a specified Fabric workspace (or an individual item, if needed) in a structured format.  

- **Activation & Inputs:** The tool can be invoked with either a Workspace identifier (`workspaceId`) to export everything in that workspace, or an Item identifier (`itemId`) to export a single item’s definition. The primary use case is full workspace export.  
- **Process:** If a `workspaceId` is provided, the tool calls Fabric’s Export Workspace API (Beta). This initiates an export of all items in that workspace. The MCP tool will wait for the API response (likely immediate if synchronous; if asynchronous, poll the operation until completion). Internally, this API assembles the definitions of all items, including their relationships, into a JSON structure (expected to be similar to what the CLI’s `fabric export` command produces).  
- **Output:** On success, the tool returns either:  
  a) The JSON payload as a string/JSON object (for the agent to potentially inspect or store); or  
  b) A reference to a file where the JSON is saved (if the content is too large).  
  We prefer returning JSON directly to keep the agent flow in-memory. The JSON will list all items with their properties, likely keyed by item type or ID. Example (pseudo-format):  
  ```json
  {
    "workspaceId": "123",
    "items": [
      { "type": "Lakehouse", "name": "SalesLake", "definition": { ... } },
      { "type": "Dataset", "name": "SalesModel", "definition": { ... } },
      { "type": "Report", "name": "SalesReport", "definition": { ... } }
    ]
  }
````

If a single `itemId` was specified, output is the definition JSON for that specific item (as today’s GET item API provides).

*   **Edge Cases & Errors:**
    *   If the workspace contains items that cannot be exported (e.g., unsupported item types or user lacks permission on some item), the tool should return a clear error. The underlying API is expected to validate permissions (user must at least have **Admin or Member** access to the workspace to export its content).
    *   If the workspace is huge, and the API times out or fails, the tool should catch that and provide a meaningful message (e.g., “Export failed due to size – consider splitting content”). However, since a single workspace’s definitions are typically not extremely large (metadata only, no actual data), we expect this to be rare.
    *   The tool does not sanitize or hide anything in the definitions – it’s a raw export. That means things like embedded credentials are not part of definitions anyway (and if they were, they’d likely come out as references or encrypted). We should confirm that sensitive info (like connection strings) come through in a secure format or require re-auth on import (which is by design in Fabric).

*Implementation Note:* Initially, the tool will likely call the **Fabric CLI** under the hood to perform the export (since the CLI team is adding support for folders and batch export). For example, it might execute `fabric workspace export --workspace <id> --includeAll` and capture the output JSON. This leverages tested CLI logic and ensures any format changes are centralized. Alternatively, the tool can call the REST API directly via an HTTP client. We’ll choose the method that minimizes errors (leaning towards CLI for convenience, as that also handles auth tokens and paging).

### **Tool 2: `ImportDefinitions`**

**Purpose:** Import a set of item definitions into a target workspace, effectively creating those items (or updating if specified). This is the reverse of Export and should consume the same format JSON produced by Export to recreate the content.

*   **Activation & Inputs:** The tool requires a `workspaceId` (target workspace where items will be created) and the JSON of definitions to import. The JSON could be provided inline (if small enough) or via a reference (perhaps the tool can accept a file path or a variable holding the JSON from a prior Export tool call). For an initial implementation, the agent will likely call ExportDefinitions, get JSON, and pass it directly into ImportDefinitions.

*   **Process:** The tool calls the Fabric Import Workspace API, sending along the definitions JSON and target workspace. The API will interpret the JSON and create all specified items. Under the hood, this might be an asynchronous job because multiple items are being created (some in parallel, some sequentially). The MCP tool should handle this by either polling until completion or by streaming intermediate updates if available (e.g., if the API returns progress events via the connection, which in local MCP context might not be straightforward – polling is fine).

*   **Order & Dependencies:** The API is expected to automatically handle ordering (e.g., create Lakehouse first, then anything dependent). If the API requires items to be sorted or tagged by dependency, the tool must ensure the JSON format is exactly as expected. (From internal info, the batch import API uses the definitions as is and the service does the heavy lifting of reconnecting references.) We should validate with the Fabric team whether any item types need special treatment. For example, if a dataset connects to an Azure Data Lake via a **Connection**, does the import API also recreate connections or expect them pre-existing? As a best practice, the environment (capacity, linked services, etc.) should be prepared beforehand or documented as a prerequisite.

*   **Output:** Upon completion, the tool returns a summary of results. E.g., how many items were created, and any warnings. Ideally, it could list the new item IDs and names. For instance: *“Imported 10 items into workspace 456: Lakehouse X (new ID abc), Dataset Y (id def), Report Z (id ghi), …”*. This gives the agent or user immediate confirmation. In case of partial failure, it should list what succeeded before the failure occurred. However, the import API likely performs an atomic operation (all-or-nothing), so failure would result in nothing created.

*   **Error Handling:** The tool should capture common errors:
    *   *Validation errors:* e.g., if the definitions JSON is malformed or refers to a resource that isn’t present (maybe an external data source not accessible, etc.). The API should return an error detailing the issue (e.g., “Lakehouse ID not found for dataset”). The tool will surface that message.
    *   *Permission errors:* if the user doesn’t have rights to create items in the target workspace (or if the workspace is in a Premium capacity requiring certain roles). The user must be at least Admin or Member on the target. The tool should detect a 403/401 response and prompt the agent to inform the user.
    *   *Conflict errors:* if the workspace already has an item with the same name or GUID and the API doesn’t allow duplicates. Since the definitions include item IDs, a conflict could occur. The new API might ignore the original GUIDs and always create new ones (expected behavior), but name collisions could still be an issue (Fabric typically allows duplicate item names in a workspace, except for unique things like workspace name or maybe Lakehouse name in Lakehouse section). We’ll confirm from API docs. Our guidance: assume target workspace should be empty for now.
    *   *Partial Import & Rollback:* if one item fails, does the API rollback all? We believe yes (transactional behavior). However, to be safe, our tool could fetch the target workspace’s item list before and after – if after failure we find some items were created, we could offer to clean them up (or at least inform the user). In initial version, we rely on the API’s behavior (which likely does rollback on failure).

*   **Optional Parameters:** While not in the first version, we anticipate the API may evolve to include options like `overwriteExisting: true` or mapping rules for certain connections. We will monitor that. For now, no extra flags beyond essential input.

*Implementation Note:* Like Export, this can be implemented by calling the Fabric CLI (`fabric workspace import` pointing to a JSON file or piped JSON). The CLI is expected to incorporate folder structure handling and may automatically handle breaking long requests into smaller ones if needed. Using the CLI helps, for example, to support scenarios like preserving workspace folder organization (the API likely supports it via parameters, which the CLI design explicitly covers in “Import/Export Folders” doc). If CLI integration proves complex, direct REST calls will be used. In either case, ensure the MCP server runs with adequate context (i.e., user credentials and environment configured) to call these operations.

### **Additional Tools (Supporting Features)**

While the spec focuses on core import/export, we propose complementary capabilities to enhance usability:

*   **`GenerateImportScript` (Future):** Instead of directly executing an import, the agent can request a script that would perform the import. For instance, this tool could output a Fabric CLI command sequence (or PowerShell script using Fabric REST SDK) that the user could run manually. This caters to power users who want the exact steps. It might produce something like:
    ```bash
    # Bash script
    fabric workspace create --name "Cloned WS" --assign-to-capacity <capID> -o newWsId
    fabric workspace export --id <sourceWsId> -o workspace.json
    fabric workspace import --id $newWsId --file workspace.json
    ```
    This is more of a convenience/documentation generator; since our main ImportDefinitions will handle execution, this is lower priority. But it aligns with the **human-in-the-loop** philosophy by showing *exactly what would be done* in a form the user can run.

*   **Rich **Validation Utility****: We could integrate a lightweight checker that runs through a definitions JSON and flags potential issues *before calling Import*. E.g., verifying that for each dataset’s data source, the target workspace has an equivalent connection or that any required Fabric capacity feature is available. Some of this is complex to generalize, but even a basic schema validation (do all items conform to known schema, are all referenced item IDs present in the payload, etc.) could prevent runtime errors. Perhaps the `ImportDefinitions` tool runs this check and warns in the output if something looks off (without blocking the import unless it’s a fatal issue).

*   **Selective Export (Item-Level):** Although full workspace is default, we might include an `itemId` filter on ExportDefinitions. This enables scenarios like exporting just one report’s definition (and possibly its dataset). However, note that exporting a single item might produce an incomplete package if that item depends on others not exported. We will clarify in documentation that for consistent results, workspace-level export is recommended. Item-level export is mainly for obtaining a single JSON (like to save a backup of one report or to copy one pipeline between workspaces).

## **Workflow and UX Considerations**

These tools are meant to be used by AI agents (Copilots) in conversational or automated workflows, with a **human approving actions**. The typical workflow might be:

1.  **Agent retrieves context:** using `ExportDefinitions`, agent gets the JSON for a workspace. The agent might summarize to the user: *“I found 12 items (1 Lakehouse, 2 Datasets, 3 Reports, etc.) in Workspace X. I have their definitions ready.”*
2.  **Agent (optional) modifies or filters:** If the user requested changes (e.g., “clone workspace but use a different data source for the Lakehouse”), the agent can edit the JSON accordingly or note that this will require a post-import step (depending on capabilities).
3.  **Agent prepares import:** The agent says *“I can now import these into a new workspace Y.”* It should either show a plan (list of items to be created) or even show a CLI script (if using the script-generation approach for transparency).
4.  **User confirms:** The user gives the go-ahead (e.g., “Yes, do it”).
5.  **Agent calls `ImportDefinitions`:** The tool executes, returning success or error info.
6.  **Agent reports outcome:** e.g., *“All items were created successfully in Workspace Y. The report ‘SalesOverview’ is now connected to dataset ‘SalesModel’ in Y.”* If there were issues, those are surfaced: *“Import failed: dataset’s data source not found in target (needs a linked connection). No changes were made.”* The user could then address the issue (create the missing connection or credential) and retry.

Throughout this, the **user remains in control**. The agent never calls Import without an explicit confirmation (this is by design; the agent’s prompting logic will enforce it, as also guided by Azure’s precedent in their Execution MCP). The tools themselves will not auto-execute on generation – they require a call, which the agent decides to do based on user input or prompt engineering.

**Dry-Run/Preview Mode:** One could imagine the agent offering *“Would you like to do a trial run?”*. If we implement a dry-run parameter, the agent could call `ImportDefinitions(dryRun=True)` which would invoke the API with a validate-only option (if available) or simulate the import. If the API doesn’t support that directly, this might be limited to our own validations. So initially, the agent might simulate a dry run simply by doing the export and analysis, but not calling import until confirmation.

**User-facing Messaging:** We will provide guidance in docs for how the agent should phrase its actions around these tools. Key points:

*   Emphasize no data is being deleted or altered in source during export (it’s read-only).
*   Clarify what the import will do, especially if the target is not empty (the agent should warn that name conflicts could occur or that it assumes workspace is empty/new).
*   Encourage the agent to show the high-level structure of the workspace to be imported (e.g., list item names) so the user recognizes the content before creation.
*   If using script generation, the agent may present the script in a code block for the user to inspect. This builds trust, as the user sees exactly the commands (some advanced users may even opt to execute them manually outside the agent if desired).

**Integration with Portal/VSCode:** In a pure chat scenario (like Teams with Copilot), the above textual confirmations suffice. In VS Code, where the developer is likely more technical, they might prefer the script approach or even directly running CLI themselves. The MCP tools are still useful in VS Code Copilot context, but maybe the AI there will more often just suggest code (e.g., Python code that calls the Fabric SDK to do import). However, since our goal is to have a unified approach, we’ll ensure the MCP tools can serve both “AI generating code” and “AI executing directly” paradigms. We expect the **Unified Creator Copilot** to use these same tools under the covers for natural language commands (“Promote my workspace to production” triggers these tools behind the scenes).

## **Best Practices & Precedents**

The design draws from internal best practices and external precedents:

*   **Bulk Definition Management:** Treating a workspace as a unit of deployment is a best practice gleaned from customers. Manual methods required exporting one item at a time via `GET /items/{id}?expand=definition` and then creating them in the right order. Our tools remove this tedium. Bulk import/export ensures **consistency** – all items come from the same point in time and their relationships remain intact. This is akin to how Azure Data Factory uses ARM templates and how Power BI introduced PBIP files for reports/datasets bundling. It reduces errors from missing a piece.

*   **Auto-Binding of Relationships:** The new Fabric API and thus our tools eliminate the manual *“find & replace GUIDs”* step that was previously necessary. As Ted Pattison noted, some item types recently gained **auto-rebind on import** (notebooks and pipelines now auto-adjust to new Lakehouse IDs). Our implementation relies on the platform’s capabilities here. If any item type doesn’t auto-bind, we will document it. (For instance, if *Notebook* import still pointed to the old Lakehouse in some preview version, the agent would need to correct it via an update call. However, by 2026 notebooks should auto-bind to the new Lakehouse.) Essentially, any gaps in auto-binding are considered bugs to fix in Fabric – the expectation is full support over time. Our spec will track known exceptions so that MCP could work around them if needed temporarily.

*   **Human Oversight & Transparency:** A major concern with AI-driven operations is trust. Feedback from design sessions emphasizes that **MCPs must be auditable and governed**. In practice, this means every action via our import/export tools should be traceable (we can rely on Fabric’s audit logs which will show who created items, or instrument the MCP to log tool usage). We won’t implement separate authentication beyond the user’s Fabric credentials, but we ensure the agent can’t bypass Fabric’s security. Also, the design encourages showing the user what’s happening (e.g., listing out created items, as mentioned). This addresses the *“predictability”* concern: users want to avoid surprises and hidden costs. By making actions explicit, we help users treat the agent’s suggestions as they would a colleague’s – to be reviewed and then trusted.

*   **Comparison with Community Solutions:** The interest in such tools is evidenced by community projects. For example, an independent developer built a custom **Fabric MCP server** that could **query and modify Power BI datasets, refresh data, and manage workspaces via AI**. Another open-source “Fabric Workspace Reader MCP” lists items and definitions read-only. These validate our approach – but since those are unofficial, they often required complex setup (client IDs, etc.) and lacked full support. We aim to offer an official, fully supported solution that covers more ground (especially the write operations). Similarly, the **Power BI PBIP** format (now part of Fabric) allows export of a report+dataset to a file, which can be imported via the Power BI REST API or UI. Our import/export is effectively “PBIP for everything, at workspace scope”. This comprehensive approach is a competitive advantage for Fabric, simplifying migration scenarios that competitors handle with heavier tooling or not at all.

*   **Integration with CI/CD**: Microsoft’s internal “fabric-cicd” Python library (now officially supported per CAT team updates and integrated into CLI) was an interim solution to programmatically deploy definitions. Our MCP tools will likely use the same underlying calls, meaning that an AI agent’s actions will be consistent with what a DevOps pipeline would do using fabric-cicd or the CLI. This consistency ensures that if an organization has both traditional CI/CD and new AI-based automation, they don’t conflict or diverge. In fact, an agent could be seen as just another way to trigger the same deployment logic.

*   **Governance & Safety Switches:** It’s worth noting that enterprise customers view MCP servers as **“delegated control points”** that should be scoped and auditable. In the context of import/export, this means:
    *   We must respect governance settings (e.g., if an admin disabled workspace creation for a user, the agent cannot circumvent that – the import tool will fail if trying to create in a disallowed capacity or if certain item types are restricted by policy like sensitivity labels on export).
    *   If needed, admins might want the ability to disable certain tools. This is outside the scope of the MCP spec (the MCP server today doesn’t have per-tool enable/disable toggles), but as a design principle, we keep these tools separate and clearly categorized as “potentially impactful”. In documentation, we’ll advise that usage of Execution tools (like Import) can be monitored via audit logs. In the future, Fabric might allow an organization to toggle agent abilities through a policy – we’ll align with any such features when they arrive.

In summary, the Import/Export tools for Fabric Local MCP will empower advanced deployment scenarios through AI, while adhering to the best practices of traditional DevOps (atomic deployments, repeatability, minimal manual steps) and maintaining the guardrails necessary for trust. By using these tools, an AI agent becomes a **DevOps assistant**, handling the mechanics of content migration so users can focus on higher-level orchestration (specifying *what* to clone and *where*, rather than *how* to script it). This spec covers the initial implementation; we will evolve it as we gather feedback from private preview and as Fabric’s APIs mature (for example, adding partial import or direct integration with source control in future versions).

## **Security, Permissions, and Audit**

*   **Authentication:** The tools execute under the credentials provided to the MCP server (the user must sign in to the Fabric MCP with their AAD account, which the agent uses). Thus, all import/export actions are done **on behalf of the signed-in user** and are subject to that user’s Fabric permissions. The MCP server will not have any elevated rights of its own. No service principals or app-only permissions are used in Local MCP (for now). This means an agent cannot access or migrate content the user couldn’t via normal means. If a user lacks access to a workspace, the export tool simply cannot retrieve it (and will throw an error); similarly, import will fail if the user isn’t allowed to create content in the target workspace or capacity.
*   **RBAC Considerations:** We assume the user is an Admin of the source workspace (to export everything) and an Admin of the target (to import). If not, partial results or permission errors may occur (e.g., a Member can usually still read all content and even export it, but a Viewer might be restricted from some definition details). In documentation, we’ll recommend that the user have Admin role for these operations to ensure completeness.
*   **Audit Log:** Actions done via these MCP tools should appear in the Fabric audit logs like any manual action. For example, importing definitions will result in many “Create <ItemType>” entries attributed to the user. There might not be a single “clone workspace” event (since that concept is at a higher level than the API), but each created item is logged. If needed, we could also log a custom audit entry for “Export workspace” and “Import workspace”, but that would likely need support from the Fabric backend. At minimum, we rely on existing logs, which meets compliance needs.
*   **No Data Exported:** It’s important to clarify that these tools deal only with *item metadata*, not actual data. For instance, exporting a Lakehouse gets you the lakehouse’s schema and settings, but not the parquet files inside it; exporting a Warehouse gets you the warehouse’s definition (DB name, config) but not the table rows. Data migration is out of scope (and typically handled via separate pipelines or backup/restore processes). This significantly lowers risk: the JSON definitions are generally small and not sensitive (aside from maybe connection strings which are often stored as secure references). However, if there are embedded credentials (e.g., a data source connection string with an inline token), the user should treat the exported JSON as sensitive. The MCP can highlight if any such fields exist by checking for known patterns (perhaps warning: *“Note: definitions include credentials for data source X in plain text”* if that scenario arises – but ideally, Fabric APIs avoid returning secrets in export, requiring re-auth on import).
*   **Safe Failure Modes:** In case of failure during import, the default stance is that nothing (or only partial) changes happen in the target. We will verify that the Import API has transactional behavior. Regardless, our tool will never try “dangerous” compensating actions without user consent. For example, we won’t auto-delete partially created items without explicit instruction. We leave any cleanup decisions to the user (or user’s agent with permission). This conservative approach avoids the AI overstepping – if something goes wrong, a human should decide how to rectify it.
*   **Misuse Scenarios:** What if a user erroneously tries to clone a huge workspace into an unsuitable environment (e.g., a workspace with premium features into a small capacity)? The attempt might strain resources or fail. We assume Fabric backend will reject or throttle if capacity can’t handle it. From MCP side, we’ll catch timeouts and failures, informing the user. We won’t impose additional size limits beyond the API’s constraints. In extreme cases, an import could potentially flood a workspace with items – the user should be aware of what they’re doing (again, an agent should communicate “we’re about to create X items”). The design’s reliance on explicit confirmation helps mitigate accidental misuse.
*   **Governance Integration:** As Fabric evolves, features like **workspace templates** or **governed environments** may provide alternatives to cloning by definitions. Our tools complement those: for instance, an admin could provide a template, but an agent might still use export/import to apply that template with AI-driven modifications. If any governance policy should restrict definition movement (for example, *“Prevent export of datasets with certain sensitivity labels”*), the Fabric API would enforce it and our tool would comply (likely via an error). We will stay aligned with any such policies, ensuring the MCP doesn’t become a loophole.

## **Conclusion**

These Import/Export tools will significantly enhance the Fabric Local MCP’s utility, enabling advanced scenarios where AI agents assist with deployment and migration tasks. By building on the robust foundation of Fabric’s batch APIs and following a human-centered approach (no silent actions, always reversible), we aim to deliver both **power and peace of mind**. The spec above covers functionality, design choices, and safeguards to implement in the upcoming development cycle.

**References (Internal Use Only for Spec Validation):**

*   Fabric Partner feedback highlighting need for easy workspace clone and confirming batch API arrival
*   Automation & Embedded PM Weekly – mentions execution MCP and batch Import/Export tool alignment with REST APIs
*   Internal planning notes (Nov 2025) – scenario of AI-driven workspace forking and CLI integration
*   Customer validation (Oct 2025) – MCP must act within governance boundaries, not replace dev pipelines but augment them.

```
```

---
---
---



# **Fabric Local MCP – Import/Export Tools Specification (Enhanced Context)**

## **Overview & Objectives**  
The Fabric Local MCP (Model Context Protocol) is a **local server** that provides AI agents with full knowledge of Fabric’s APIs, item schemas, and best practices, **without directly executing changes** in the tenant[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/)[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/). It currently runs in a read-only mode: agents can retrieve information (like item JSON definitions or API specs) and generate code suggestions, while **humans remain in control** of running any actions[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/)[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/). 

**Goal:** Extend the Local MCP with **Import and Export tools** to manage item definitions (reports, datasets, pipelines, etc.), enabling agents to **export** these definitions to structured files and **import** them into Fabric – effectively automating content migration – under user supervision. The design must ensure **reliability (preserve dependencies, validate data)** and **user trust (explicit confirmation, no surprise changes)**.

By introducing these tools, we’ll shift Fabric MCP from a purely advisory role to a **hybrid role**: still primarily offering context and code generation, but now also capable of carrying out user-approved actions. This is in line with internal plans to add **execution capabilities** to MCP servers, carefully gated by user input[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D).

## **1. Capabilities of Fabric MCP for Item Definitions**  
**Current State (Preview):** Fabric MCP exposes a **JSON schema for every Fabric item type** (Lakehouse, Pipeline, Dataset, Report, etc.), allowing agents to understand the structure of item definitions[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/). Agents can list workspaces, fetch item metadata, and get item definitions. However, they **cannot modify or create items** directly – they only produce code or HTTP requests that the user may run elsewhere[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D)[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D). The MCP is essentially a **“read-only” server** with all the info needed to construct or update items, but it leaves execution to the user[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/)[1](https://blog.fabric.microsoft.com/en-us/blog/introducing-fabric-mcp-public-preview/).

**Planned Enhancements:** The roadmap involves enabling certain **write operations** in a controlled fashion. As of late 2025, the team committed to adding tools for execution, starting small with local tasks (e.g., uploading a file to OneLake)[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D). For core content actions (creating workspaces, items, etc.), they’re designing a strategy to share logic with a **remote “Execution” MCP** that’s being developed[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D). In practical terms, this means the Local MCP may call the same routines or APIs as the Remote MCP when doing things like creating items, ensuring consistency across local and cloud agent experiences.

Specific to **item definitions**:  
- *Documentation & Schema Support:* The MCP is incorporating detailed documentation for item definitions. For example, internal plans mention adding **embedded item definition articles** as part of MCP’s context[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). This will enrich agents’ understanding of each JSON field (beyond just the schema) and help them make valid changes.  
- *Partial Updates:* Currently, item definitions are handled as whole JSON blobs. The team recognizes a need for more granular tools (e.g., update just a report title or a measure). A separate effort in the “report builder” group is prototyping a **report MCP** which can manipulate the Power BI report definition model via a more structured object model[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). The intent is to generalize that approach so that *all item types* (not just reports) can support partial edits via MCP[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). This could mean introducing tools like `UpdateReportDefinitionSection`, or adopting a unified method to apply JSON-Patch style updates to item definitions. While this is still in early discussion, it’s noted as a requirement to avoid building one-off solutions per item type[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). For now, our import/export tools will operate at the full-definition level, but we keep in mind that future enhancements might make interacting with definitions more fine-grained.  
- *Large Definition Handling:* A challenge identified is that some item definitions can be extremely large (e.g., a complex Power BI dataset with hundreds of measures). These can exceed token limits for AI models or even cause performance issues in the MCP. It was decided that for the immediate term, **documenting a limitation for large definitions** is necessary[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D)[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). Agents might not be able to intelligently modify such definitions inside the chat (the AI model could struggle), so the MCP might need to provide them as files or only allow export/import as whole units. One idea floated was using Fabric notebooks to run CLI commands for large content, bypassing the need for the AI to ingest the entire JSON[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). For instance, if a dataset definition is huge, the agent could call an MCP tool that executes a CLI operation (in a notebook context) to update a specific part, rather than trying to load it all into the prompt. This is a complex area, but the takeaway is: **the spec should note potential constraints on handling very large definitions**, and possibly include a fallback mechanism (like splitting the definition or using external script execution) if needed.  
- *Read-Only vs Write Tools:* Each MCP tool will be tagged to indicate whether it’s read-only or can perform writes. For public preview, the team plans to mark all new execution tools with a “read-only: false” annotation so that AI agents are clearly aware which tools can change state[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D). This affects how agents decide to use them; they typically seek user confirmation before using non-read-only tools (per protocol guidelines). In our spec, we should define `ExportDefinitions` as a read-only tool (no changes, just data retrieval) and `ImportDefinitions` as a write tool (it creates resources). This distinction will also be documented in tool descriptions.

Summarily, the Fabric Local MCP is evolving from **just a knowledge source to a controlled actor**. It will maintain the principle that **“nothing happens unless the user approves”** (as emphasized in security reviews[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D)[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D)), but it will streamline the steps between code generation and execution by having built-in tools to do tasks like import/export when asked. The import/export tools are among the first significant “write” capabilities, so they will set precedents for how we ensure safety and correctness in MCP-driven changes.

## **2. Fabric Import/Export Batch APIs – Technical Integration**  
Implementing import/export in MCP will heavily leverage Fabric’s upcoming **Batch Import/Export REST APIs**. These are new endpoints designed to export an entire workspace’s definitions and import them into another workspace in one operation. 

**API Design:** The Batch Export API returns a JSON structure that encapsulates all items in a workspace (except maybe unsupported item types, see below). The Batch Import API ingests such JSON and reproduces the items in a target workspace. Internally, these are long-running operations that handle sequencing and dependency resolution. In private previews (as of Dec 2025), the API had some limitations:  
- **Lakehouse Support:** Initially, Lakehouse items could not be created via definitions (the workload team hadn’t finished the feature). In fact, as of Dec 2025, the import API would **not accept Lakehouse items**[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1)[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). This was a known blocker since Lakehouses often have many downstream dependencies and needed first-class support. By late Jan 2026, this was resolved at the base API level: *“Create Item now supports creating Lakehouse with Item Definition (documentation added)”*[5](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7BAF32A3F7-0984-4FBA-AA3C-8304456311A7%7D&file=ISV-Developer-Sync-2026-01-27.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). So the Import API should soon handle Lakehouses as well. Our MCP implementation will assume that by GA, Lakehouse definitions are supported. However, in early testing, we need to account for scenarios where a workspace includes a Lakehouse: if the platform still rejects Lakehouse via Batch Import, the MCP might need to warn the user or implement a fallback (e.g., skip the Lakehouse and prompt the user to create it manually first). We expect this to be a temporary workaround since full Lakehouse support is imminent[5](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7BAF32A3F7-0984-4FBA-AA3C-8304456311A7%7D&file=ISV-Developer-Sync-2026-01-27.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1).  
- **Service Principal (SPN) Authentication:** The private preview API did not support service principals (app-only calls)[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). In our context (Local MCP with a signed-in user), this is not a problem – the MCP will call the API using the user’s credentials/token. But it’s noteworthy for background automation scenarios. Eventually, SPN support will matter for headless CI pipelines, but since our MCP is user-driven, we just ensure the user’s identity has rights to source and target workspaces.  
- **Performance & Reliability:** Early tests flagged performance issues when exporting very large workspaces, which delayed the preview’s release[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). The API presumably had to serialize lots of JSON and might time out for big workspaces. They were working on optimizations (maybe streaming or pagination of the output). By Jan 2026, internal status shows *“Import/Export Batch Fundamentals – in progress”* with focus on LRO (long-running operation) items and permission validation[6](https://microsofteur.sharepoint.com/teams/TridentPublicAPIsauthoring/_layouts/15/Doc.aspx?sourcedoc=%7B7E81767D-C258-43D7-8FD4-C8A03C107F2D%7D&file=Fabric%20Automation%20Status.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1)[6](https://microsofteur.sharepoint.com/teams/TridentPublicAPIsauthoring/_layouts/15/Doc.aspx?sourcedoc=%7B7E81767D-C258-43D7-8FD4-C8A03C107F2D%7D&file=Fabric%20Automation%20Status.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1), suggesting improved handling of large jobs. The MCP should be ready to handle the case where an export or import call takes a while: possibly by showing progress or at least not blocking other interactions. If the API is truly asynchronous (e.g., returns a job ID), we’ll implement the tool to poll for completion and only return when done, to simplify the agent’s logic.  
- **Payload Contents:** The export JSON includes all item definitions and likely also **workspace-level settings** (like workspace name, maybe default capacity or linked capacity ID, etc.). It also should include **folder structures** within the workspace. Fabric recently introduced workspace folders for organizing items; the CLI team has explicitly worked on preserving folder paths in exports and imports[7](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQuc2hhcmVwb2ludC5jb20vY29udGVudHN0b3JhZ2UvQ1NQXzc4MDE2M2I3LWNiMjMtNDdjMi04MWFhLWZiYzdjYjJkMGE0OD9uYXY9Y3owbE1rWmpiMjUwWlc1MGMzUnZjbUZuWlNVeVJrTlRVQ1UxUmpjNE1ERTJNMkkzSlRKRVkySXlNeVV5UkRRM1l6SWxNa1E0TVdGaEpUSkVabUpqTjJOaU1tUXdZVFE0Sm1ROVlpVXlNWFF5VFVKbFExQk1kMnRsUW5GMmRraDVlVEJMVTBab1RHSTRjWGhFV1hCR2JqWWxNa1JOV2tZMkpUVkdTbmxOT1c1aU0xVnRWV0k1VTBwbk9VRkhUR1ZNV1VSV0ptWTlNREUyVjFGR1VWSkNUMEpVUlZKQ1QxbFNOa0pHV1UxSVNVTklRMVpaUnpWRU5TWmpQU1V5UmcifQ%3D%3D)[8](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQuc2hhcmVwb2ludC5jb20vY29udGVudHN0b3JhZ2UvQ1NQXzc4MDE2M2I3LWNiMjMtNDdjMi04MWFhLWZiYzdjYjJkMGE0OD9uYXY9Y3owbE1rWmpiMjUwWlc1MGMzUnZjbUZuWlNVeVJrTlRVQ1UxUmpjNE1ERTJNMkkzSlRKRVkySXlNeVV5UkRRM1l6SWxNa1E0TVdGaEpUSkVabUpqTjJOaU1tUXdZVFE0Sm1ROVlpVXlNWFF5VFVKbFExQk1kMnRsUW5GMmRraDVlVEJMVTBab1RHSTRjWGhFV1hCR2JqWWxNa1JOV2tZMkpUVkdTbmxOT1c1aU0xVnRWV0k1VTBwbk9VRkhUR1ZNV1VSV0ptWTlNREUyVjFGR1VWSkVNbGRGUjFOTVJVVlNOMHBHTWsxVVN6WkpTVUUxUjA5Q1ZTWmpQU1V5UmcifQ%3D%3D). We will ensure our tools support that – i.e., the JSON will contain folder info and the Import tool should recreate the same folder hierarchy in the target workspace.  
- **Exclusions:** Some items might not be fully covered by the batch API yet. For example, **Paginated Reports (RDL)** were noted as lacking Fabric-native APIs; they rely on Power BI endpoints[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1)[9](https://microsofteur.sharepoint.com/teams/PBIALM/_layouts/15/Doc.aspx?sourcedoc=%7BB080436F-F05D-4AF2-9205-F788F7818C8A%7D&file=CICD%20paper%20cuts.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1). If those aren’t included in the batch export, we should at least call it out. (Maybe the batch export would skip them or include a pointer to a .RDL file if one is attached? This needs confirmation.) Another possible gap: **Connections and Linked Services**. Workspace connections (to external data sources) are separate objects in Fabric. Ideally, the batch export should include them so that datasets or pipelines that rely on connections can be imported without manual re-creation. The design likely does include connections, as they are part of the workspace metadata. We should verify this during testing; if not included, an import might create a dataset that points to a connection ID that doesn’t exist in target, causing failure. In such a case, the Import tool would catch the error and prompt the user to create necessary connections first (or in future, MCP could have an `ImportConnection` sub-tool).  
- **Automatic ID Mapping:** The whole point of the batch API is to handle inter-item references. In older manual processes, after exporting definitions one had to manually replace resource IDs (like datasetId, lakehouseId) inside the definitions with the new ones. The batch Import API is documented to do this automatically. Specifically, when you import the JSON, the system will generate new GUIDs for each item and update any references in the payload to those new GUIDs. For example, if a report’s definition in the JSON references dataset GUID `A` (from source workspace), and in the import process the dataset gets a new GUID `A*`, the report is created referencing `A*`. Our MCP tools thus don’t need to do any ID manipulation – they just feed the JSON in and let the backend do the rest. We should confirm through testing: e.g., create a workspace with a dataset and report, export, import to new workspace, and verify the new report points to the new dataset (and not some missing ID). Based on internal PM confirmation, this is indeed handled: *“the same relations between workspace items”* are established by the import operation.  
- **State of Imported Items:** The import process likely creates items in an *unpublished* or *inactive* state where applicable. For instance, if a pipeline is imported that had a schedule, that schedule might need reactivation. Or a dataset might need credentials or refresh. The API might not carry over things like refresh history obviously, but it might carry over schedule definitions (disabled until configured). We’ll note in documentation that after an import, some items (like datasets or pipelines) may require additional setup (such as re-entering credentials for data sources, as credentials typically don’t export, or re-establishing some triggers).

**Integration Approach in MCP:** We have two primary ways to call these APIs: directly via HTTP from the MCP server, or through the Fabric CLI (which will include these commands). We’re leaning towards using the **Fabric CLI** internally for a few reasons:  
- The CLI provides a convenient wrapper and might handle things like chunking large JSON and waiting on operations. It also uses the user’s logged-in session (or prompts for auth if needed) – but since the user already authenticated the MCP (likely via device login), we can possibly piggyback on that token.  
- The CLI team is actively developing bulk export/import and ensuring they work end-to-end, including file handling and folder preservation[7](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQuc2hhcmVwb2ludC5jb20vY29udGVudHN0b3JhZ2UvQ1NQXzc4MDE2M2I3LWNiMjMtNDdjMi04MWFhLWZiYzdjYjJkMGE0OD9uYXY9Y3owbE1rWmpiMjUwWlc1MGMzUnZjbUZuWlNVeVJrTlRVQ1UxUmpjNE1ERTJNMkkzSlRKRVkySXlNeVV5UkRRM1l6SWxNa1E0TVdGaEpUSkVabUpqTjJOaU1tUXdZVFE0Sm1ROVlpVXlNWFF5VFVKbFExQk1kMnRsUW5GMmRraDVlVEJMVTBab1RHSTRjWGhFV1hCR2JqWWxNa1JOV2tZMkpUVkdTbmxOT1c1aU0xVnRWV0k1VTBwbk9VRkhUR1ZNV1VSV0ptWTlNREUyVjFGR1VWSkNUMEpVUlZKQ1QxbFNOa0pHV1UxSVNVTklRMVpaUnpWRU5TWmpQU1V5UmcifQ%3D%3D)[8](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQuc2hhcmVwb2ludC5jb20vY29udGVudHN0b3JhZ2UvQ1NQXzc4MDE2M2I3LWNiMjMtNDdjMi04MWFhLWZiYzdjYjJkMGE0OD9uYXY9Y3owbE1rWmpiMjUwWlc1MGMzUnZjbUZuWlNVeVJrTlRVQ1UxUmpjNE1ERTJNMkkzSlRKRVkySXlNeVV5UkRRM1l6SWxNa1E0TVdGaEpUSkVabUpqTjJOaU1tUXdZVFE0Sm1ROVlpVXlNWFF5VFVKbFExQk1kMnRsUW5GMmRraDVlVEJMVTBab1RHSTRjWGhFV1hCR2JqWWxNa1JOV2tZMkpUVkdTbmxOT1c1aU0xVnRWV0k1VTBwbk9VRkhUR1ZNV1VSV0ptWTlNREUyVjFGR1VWSkVNbGRGUjFOTVJVVlNOMHBHTWsxVVN6WkpTVUUxUjA5Q1ZTWmpQU1V5UmcifQ%3D%3D). Using the CLI means reusing that tested logic. For example, rather than coding our own polling for the LRO, the CLI might do it and just output final success or errors.  
- Output format: The CLI could directly save to a file. However, for the MCP’s purpose, we might prefer to get the JSON in-memory (so the agent can use it, maybe modify it or store it temporarily). If the CLI doesn’t support printing JSON to stdout, we might call the REST API directly to retrieve it. Another idea is to call CLI to save to a temp file, then MCP reads the file content and returns that. This would all be abstracted in the `ExportDefinitions` tool (so the user/agent doesn’t see these intermediate steps).  
- **Dependency**: We will note that using the CLI requires the CLI to be installed or accessible by the MCP host. Since both are part of Fabric’s developer tools, this is a reasonable assumption. The CLI is open-source and can be bundled or installed via pip (as a Python CLI) or as a dotnet tool. The MCP could, for instance, call a Python subprocess for `fabric export ...`. Alternatively, since the MCP server itself is .NET, it could reference the same libraries the CLI uses (if available as NuGet) to call the APIs directly. We’ll assess which approach keeps maintenance simpler. Possibly calling the REST API via HTTP client (since we have access to token) might be straightforward, and we then manually handle writing a file for folder structure if needed (the JSON likely includes folder path info for each item, so we just pass it through).

**Tool Implementation Summary:**  
- **ExportDefinitions Tool:**  
  - *Input:* `workspaceId` (string GUID or potentially a friendly name, but GUID is safer to avoid ambiguity) and optional `itemId` (if exporting a single item). Another optional parameter could be a list of item types or an inclusion filter, but the primary use is full workspace.  
  - *Operation:* If `itemId` provided, call existing GET item definition API (which returns that item’s JSON). If only `workspaceId`, call the Batch Export API for the workspace. This might be an HTTP POST like `/api/workspaces/{id}/Export` that returns a JSON document. In case it’s async, the call might return a job ID and a URL to poll – the MCP will handle the polling and fetch the final JSON when ready.  
  - *Output:* Return the JSON string (or structured object). Since the agent might not need to read or modify it extensively (often it will pass it directly to import), returning it as an opaque string is fine. However, we might also consider writing it to a local file (like `FabricExport_<wsName>.json`) and returning a reference path. But that adds file management for the user. Most likely, the agent itself will store this JSON in memory (or as an “attachment” in the conversation context). We will specify that the JSON can be quite large, so memory impact should be considered. If it’s huge (say > a few MB), writing to a file and giving the agent a file reference (like `#file` ID in the chat) might be more stable. We can support both modes: by default, return JSON as text up to a size limit, beyond which we fallback to file and inform the user/agent.  
  - *Error cases:* If workspace not found or no access, error out with message. If the workspace contains any not-yet-supported items, the API might return an error or partial data. It’s preferable the API errors (so user knows something didn’t export). We will propagate that error. For example, if the API says “Paginated report not supported”, the tool can surface: *“Export failed: item type 'PaginatedReport' in the workspace is not supported by export API.”* The user could then choose to exclude that item or wait for support. For MCP’s first version, we won’t implement partial export (it’s all or nothing per workspace call).  
  - *Permissions:* The user must be an Admin or Member of the workspace to export all content. If they are Contributor or Viewer, they might lack rights to some items (Contributors can’t access all items’ definitions, e.g., maybe cannot see a dataset’s definition if they only have build permission? Actually build might allow viewing dataset schema, but not sure about full JSON). We will likely require Admin for simplicity. If not Admin, we’ll warn that results may be incomplete or just rely on the API’s permission checks.  

- **ImportDefinitions Tool:**  
  - *Input:* `workspaceId` (target workspace where to import) and either a JSON payload or a reference to one. The agent will typically get the JSON from Export and pass it in. We need to be careful about how to pass potentially very large JSON through the function call interface. If the agent has it as a string, that could blow up the message size. We might instead have the agent supply a reference like `#file` (if the agent wrote the JSON to a file via some mechanism or the user attached it). Since our Export tool is in-process, we can design an **MCP-internal caching**: e.g., the Export tool returns an identifier token (like an index or a handle) instead of raw JSON, and the Import tool can retrieve the content by that handle. However, the current MCP interface may not support persistent handles easily across tool calls (other than having the agent hold the data). We can simplify: for now, assume the agent will request to export and then immediately call import, likely without heavy modification in between. The data can be stored transiently in the agent’s state (some clients do track conversational attachments). We will clarify that if the JSON is extremely large, using a file is recommended. Possibly, we could add an argument to Export like `saveToFile: true` that saves it to, say, `./export.json` and then the agent can call Import with something like `filePath: "./export.json"`. This might be easier on the agent. The spec can mention this as a supported option.  
  - *Operation:* The tool will call the Batch Import API. Usually it requires specifying the target workspace in the URL or body. We must ensure the target workspace exists (the user might have created an empty workspace for this purpose). The import call might also require specifying if it should override existing items or not. In preview, since duplicates weren’t handled, they likely expect an empty workspace to import into (the slides emphasize cloning to a new workspace)[5](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7BAF32A3F7-0984-4FBA-AA3C-8304456311A7%7D&file=ISV-Developer-Sync-2026-01-27.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). We will **assume target must be empty** for now and document that. If not empty, either the API will create duplicates (two items with same name) which is probably not desired, or it will error if matching names/IDs found. We can do a pre-check: list items in target, and if any exist, warn or require a flag like `forceImport`. For safety, default behavior is to not import into non-empty workspace to avoid confusion.  
  - The API call again might be async (likely more so than export, because it may start background jobs to create each item). We’ll manage that by polling. Fabric’s long-running operations usually return a `operationId` that you can GET for status. The MCP can periodically poll (e.g., every few seconds) and gather progress. If partial progress data is available (like how many items done), we could relay it via tool output or logs, but the simplest is to wait until complete then return a summary.  
  - *Post-creation Steps:* The API should finalize all relationships. Some specific behaviors: If the source workspace had certain features enabled (like Git integration or sensitivity labels), those might not directly copy. We expect only items and their definitions are handled. After import, certain items might require activation: e.g., **Datasets** – if they were in Import mode storage mode, the new dataset will be empty of data until refreshed. It might also not have credentials for data sources (security measure: likely the connections are created but in a state requiring admin to reauthorize). The MCP should detect if any refresh or credential actions are needed. Perhaps in the import result, the API could flag items that need attention (not sure if it does). Otherwise, our Copilot agent (not the tool itself) can advise something like “Dataset X was imported. You may need to update its data source credentials and run a data refresh.” We will include that guidance in user-facing documentation rather than the tool logic.  
  - *Output:* On success, return a structured summary: e.g., list of items created (name, type, new ID). This is helpful for logging and for the agent to possibly mention results. If an error occurred, return the error message. If the API partially created items and then failed, it might roll back everything (expected). But if not (imagine it created 3 items and the 4th failed and it stops without rollback – though hopefully it does rollback), the tool should inform what was done. In worst case, we could attempt to clean up partial imports: however, automatic cleanup is risky (the error might be minor, and user could fix and retry without wanting to start over). It’s safer to leave any partial content with the user aware, and they can manually delete or we can provide a separate `RollbackImport` tool if absolutely needed. Initially, assume atomic import (all or nothing). We will confirm this in testing.  
  - *Edge cases:* If the target capacity or environment lacks a feature needed by an item, the API might fail. E.g., trying to import a Real-Time Analytics item into a workspace on a capacity that doesn’t support it. The error might be “item type not allowed” or similar. The MCP tool will bubble that up. We can’t magically solve it, but at least inform clearly (“Target capacity does not support item X; import aborted”). Another edge: naming conflicts – if an imported item’s name matches something already in target, and the API chose to proceed by creating duplicate, user might have two “Sales Report” items. This ties back to our empty workspace recommendation.  
  - *Permissions:* The user must have permission to create content in the target. That usually means Member or Admin role in the workspace (Viewer obviously can’t add items). Also, some tenants might have governance like requiring certain licenses or certain capacities. If an import tries to create items on a capacity where user doesn’t have Build permission or where certain item creation is blocked by policy, those would error. E.g., if an admin toggled “don’t allow dataset creation in this workspace,” the API would fail for the dataset portion. We rely on Fabric’s own error messages in such cases – MCP will just pass them on.  
  - *Concurrent Imports:* Unlikely a user would run two imports simultaneously to same workspace, but if so, it’s at their risk – the API (and underlying service) probably serializes operations per workspace. We don’t explicitly guard against that beyond whatever the service does.

**Dependency Handling in Integration:** Because the Batch Import API handles ordering, the MCP doesn’t have to individually call sub-operations. In contrast, if we were to implement import without this API, we’d have to do something like: create Lakehouse, take new ID, replace in other definitions, create next item, etc. The one scenario where we might still do something custom is if a certain dependency isn’t handled by the API at present. For example, **circular references via shortcuts** between Lakehouses (each lakehouse contains a shortcut to the other) might confuse the importer. If this scenario isn’t supported, one approach is to break the cycle manually: import one Lakehouse first (with its shortcut maybe omitted), then import the other and then perhaps update the first’s shortcut. But this is extremely edge. We will mention it as a known limitation rather than implement a complex fix. The CI/CD team noted this case and they had to deploy lakehouses and their shortcuts in separate phases manually[9](https://microsofteur.sharepoint.com/teams/PBIALM/_layouts/15/Doc.aspx?sourcedoc=%7BB080436F-F05D-4AF2-9205-F788F7818C8A%7D&file=CICD%20paper%20cuts.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1). We expect the Batch API either doesn’t allow circular shortcuts or would require a subsequent fix. Our spec will include: *“Lakehouse shortcuts that reference other Lakehouses may need a second import pass or manual adjustment.”* This at least sets the expectation for such odd cases.

**Testing Plan:** As part of development, we will test the export/import tools with various workspace configurations: a simple workspace (few items), one of each item type, one with nested folder structure, one with cross-dependencies, etc. We’ll also simulate failures (lack permissions, or attempt import with missing connection). These tests ensure the tools respond with clear messages and don’t cause data loss. We’ll also verify that after an export-import round trip, the source and target workspace are functionally equivalent (except for things like data content). For instance, run a pipeline in source and target to confirm it still works, or open a report to see if visuals load (assuming data source connections are valid).

To implement this in code within the MCP repository, we will likely add a new controller (or extend the existing one): e.g., `WorkspaceController` might get an `ExportDefinitions` endpoint that triggers the logic. The CLI integration might require invoking a shell command from .NET code; we can use `System.Diagnostics.Process` to run `fabric workspace export ...`. If the CLI is not found, the tool can fallback to direct REST calls (using an HttpClient with the user token, calling the documented REST endpoint from the Swagger spec[10](https://dev.azure.com/powerbi/3a3467dc-0814-4e9d-8eec-555851655f69/_workitems/edit/1475758)[10](https://dev.azure.com/powerbi/3a3467dc-0814-4e9d-8eec-555851655f69/_workitems/edit/1475758)). We’ll ensure any required headers or payload formatting (likely content-type `application/json`) are handled.

In summary, integrating the Batch Import/Export API into MCP means our tools act mostly as **thin wrappers** around these powerful backend operations. This gives us confidence in correctness (since the heavy lifting is server-side) and lets us focus on user experience aspects like parameter passing, error messaging, and confirmation.

## **3. Definition Export/Import Workflow – Best Practices**  
Managing item definitions effectively requires following best practices to ensure consistency and avoid errors. Our design aligns with these practices, whether drawn from internal experience or analogous platforms:

- **Workspace-Level Operations:** Always treat a set of related items (the contents of a workspace) as a unit when cloning or migrating. Exporting/importing everything together greatly reduces the chance of missing a dependency. It’s also transactional. Internally, this approach is moving from manual scripting to the new API for exactly these reasons[5](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7BAF32A3F7-0984-4FBA-AA3C-8304456311A7%7D&file=ISV-Developer-Sync-2026-01-27.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1). Therefore, our primary workflow is “export entire workspace” and “import into empty workspace.” We will still allow single-item export for edge cases (like backing up just one pipeline, or copying a specific report), but we’ll document that when using those, the user must handle dependencies manually (e.g., if exporting a report individually, the report JSON by itself is not usable unless the target has the dataset it refers to). The agent, if asked something like “copy just this report”, could choose to either export the whole workspace or at least include the dataset too, or warn the user.  
- **Preserve Relationships:** The import tool must ensure that after import, **all references are intact**. Thanks to auto-ID mapping, a dataset will have a new ID and all reports now point to that new ID automatically. One area to pay attention to is **external references**. For example, if a Dataflow in the workspace connects to a Lakehouse in another workspace, the definition might contain the original workspace and lakehouse ID. The Batch API might not fix references that point outside the payload (since it only knows about the items in the JSON). In such cases, the agent should call out that *“Remapped internal references, but external references (like data sources in other workspaces or org-level connections) need manual update.”* A best practice is to minimize such external dependencies when using this tool, or include those other items in the migration if possible. However, migrating cross-workspace links is tricky. Our spec will recommend that either those links remain pointing to the original (if that’s intended), or the user must adjust them separately.  
- **Ordering & Atomicity:** Historically, scripts had to create items in specific order (e.g., make Lakehouse first, then dataset, then report). With Batch Import, the service implicitly handles order. The best practice of “items should be created in an order satisfying dependencies” is now implemented for us. We nonetheless will double-check if any particular ordering issues remain (like the Lakehouse shortcuts scenario above). The expectation is that the import is effectively atomic – if any item fails to create, the whole import is rolled back (this was not explicitly stated in docs, but it’s typical of such batch operations). We will confirm and, if not atomic, consider implementing a partial cleanup or at least highlight to user *“some items may have been created before failure”*. Ideally we avoid partial state.  
- **Validation Before Execution:** A dry-run or validation step can save a lot of headache. While the API itself might not have a dry-run mode as of preview, our agent can simulate one by analyzing the definitions. Best practices for CI/CD often include validating templates before applying them. We will implement in the agent (not necessarily as a separate tool, but as logic) to check a few things before calling Import: 
  - Does the target workspace exist and is it empty? 
  - Does the user have the needed roles? (We can check if user is admin of target if the situation allows via Graph API or simply attempt and handle failure.)
  - Are there obvious external references? For example, search the JSON for any `"workspaceId"` that is not the source or target’s ID (meaning a link to something else). Or find any place the JSON says `"connectionId": "<GUID>"` where that connection GUID won’t exist in target. We could warn about those.  
  - If we incorporate a `ValidateDefinitions` tool, it would feed the JSON into Fabric’s definition validation logic (maybe via a hidden API). Another approach: attempt an import into a *temporary sandbox workspace* that is auto-deleted, just to see if it fails. That’s heavy-handed and likely unnecessary. Instead, rely on the real import with user awareness that it might fail partway but rollback. So the net effect is the same as a dry-run (if it fails, you’re back to original state).
  - For now, our spec will emphasize manual review: e.g., after export, an agent could present a summary of what will be imported. This summary (list of items, types, maybe key properties like row count or data volume if known) acts as a “plan preview” for the user to confirm. 
- **Post-Import Verification:** After an import, best practice is to verify the content in the target. The agent could assist with a quick verification routine (like open the new workspace, list items, maybe ensure each has the expected subcomponents). But this is outside the immediate spec for the tool itself. However, we note that the tool returning a list of created items helps with verification. The user or agent can cross-check this against the source list to ensure none are missing.  
- **Source Control of Definitions:** Even though not directly an MCP feature, it’s worth noting: exported definitions can be saved in a repository for version control. Many teams treat these JSONs as code. We encourage users (and agents, if applicable) to store the exported JSON file if this is a planned deployment – it provides an audit trail of what was moved. The agent could optionally commit the file to a Git (if working in environment with Git access). That’s beyond our tools, but our design (especially if we allow `ExportDefinitions` to save to a file path) facilitates this practice.  
- **Parameterization for Different Environments:** The **Variable Library** in Fabric is intended to handle environment-specific parameters (like connection strings, dataset parameters, schedule differences). It’s not fully integrated across all items yet[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1)[4](https://microsoft-my.sharepoint.com/personal/edpattis_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B7BC4F204-47B5-4B9A-9992-60D4B50F0E79%7D&file=ISV-Developer-Sync-2025-12-16.pptx&action=edit&mobileredirect=true&DefaultItemOpen=1), but it’s on the roadmap. Until then, if a user needs to change certain values when moving from one environment to another (e.g., use a different storage account in prod), they have to edit the JSON or adjust after import. Best practice is to minimize such differences or script them. For instance, one could export, then do a search/replace on the JSON for a dev data source ID to a prod data source ID before importing. We could have the agent handle that if instructed (maybe a future tool `TransformDefinitions` that applies simple substitutions or uses the variable library to replace tokens). For now, we mention in documentation: *“Review the definitions for any environment-specific references (like dataset connection endpoints, pipeline trigger frequencies). Adjust these in the JSON or after import. In the future, Fabric’s variable library will streamline such parameterization.”*  
- **Rollbacks:** In rare cases of error mid-import, the safest practice is to revert to the original state. If the import API is not atomic, the MCP should either attempt to delete whatever partial items were created or advise the user to delete them. Since multiple item creation via API typically either fully fails or partially succeeds, we plan on the assumption of full failure = no items. If that assumption is wrong in preview, our tool will implement a caution: on failure, it will list any items it detects in target that weren’t there before and recommend deletion. The user (or agent) can then remove them. This ensures a clean retry. Subsequent updates to the API might handle rollback internally, making this moot.  
- **Testing in non-production first:** As with any deployment, practice the export/import in a test workspace before running in an important prod workspace. That’s more a user guideline than a spec item, but it’s a best practice to call out. We can incorporate that advice in our user guide for these tools.

By building the tools to closely follow these best practices (bulk operations, built-in dependency resolution, etc.), we both simplify the agent’s logic and reduce the burden on the user to understand all the intricacies. In fact, a key benefit of using an AI agent with these tools is to hide the complexity – the agent can know them internally. But for specification completeness, we codify them here.

## **4. Human-in-the-Loop UX & Tooling Patterns**  
Since these import/export tools empower AI agents to make changes in Fabric, the **user’s oversight and control** are paramount. The UX flow must instill confidence and allow intervention at key points. Here are the patterns and features to achieve that:

- **Explicit User Confirmation:** The MCP can expose powerful actions, but the agent should never execute them without user consent. This will be enforced both by policy and by prompt design. Azure’s Execution MCP (analogue in Azure services) requires user confirmation for any operation that writes data[3](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQtbXkuc2hhcmVwb2ludC5jb20vcGVyc29uYWwvbWVkaWFvX21pY3Jvc29mdF9jb20%2FbmF2PWN6MGxNa1p3WlhKemIyNWhiQ1V5Um0xbFpHbGhieVUxUm0xcFkzSnZjMjltZENVMVJtTnZiU1prUFdJbE1qRjNRbXRhV1RNeGNVcFZUMEp3ZDJOUFNEWk9VRk53UXpKRk5YaHVTVkY0UVdjNGFXVkNSR05tTUU0bE5VWlhlQ1UxUm5kbFR5VTFSbFJSVTBwNWVVTkZVMFo1UlhwU0ptWTlNREZJUjBOSVNqTlFWa2xRV2xZMVJETlFOVUpIUzFnelEwdEZORE5MVTB0U1Z5WmpQU1V5UmcifQ%3D%3D), and we will mirror that. Concretely, when an agent is about to call `ImportDefinitions`, it should first present to the user something like: *“I can now import these 10 items into workspace X. Do you want me to proceed?”* Only if the user says yes (or chooses a UI button) will the agent call the tool. The tools themselves might also implement a safeguard: e.g., we could have `ImportDefinitions` tool ignore calls unless a flag `confirm=true` is passed, effectively requiring the agent to explicitly set that after getting consent. (This is an idea for preventing accidental use, but probably the conversation protocol level is sufficient.)  

- **Pre-Execution Plan Summary:** Before performing the import, the agent should summarize what will happen. This is akin to a deployment plan. For example: *“Plan: Create 1 Lakehouse, 2 Datasets, 3 Reports, and 1 Pipeline in the target workspace.”* The agent can get this from the export JSON (count the items by type) easily. It might also mention if any items will overwrite (though we expect none if workspace empty). By seeing this, the user can catch if anything is unexpected (e.g., “Wait, why 2 datasets? I expected 1.”) and abort or adjust. This step addresses the transparency concern where users don’t want the AI to do things behind the scenes.  

- **Script Generation Option:** Taking inspiration from user feedback and our conversation with Rui Romano, providing a **human-readable script** of the actions can increase trust[2](https://teams.microsoft.com/l/message/19:1135d11b-8a63-4009-a115-30cede516983_ca27cead-bd8a-4c09-9bd2-711f47e55864@unq.gbl.spaces/1763045043393?context=%7B%22contextType%22:%22chat%22%7D). Instead of (or in addition to) performing the import directly via API, the agent could generate a CLI or PowerShell script that the user can review. For instance, the agent might say: *“Here is a Fabric CLI script to clone the workspace. You can run it yourself or let me execute it:”* and present something like:  
  ```bash
  fabric workspace create --name "Target Workspace" --capacity P1 --out targetId 
  fabric workspace export --id <sourceId> --file export.json 
  fabric workspace import --id $targetId --file export.json